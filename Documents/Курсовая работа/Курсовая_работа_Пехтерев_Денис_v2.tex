\documentclass{mais}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[mag=1000,a4paper,left=2.2cm,right=3.0cm,top=3.1cm,bottom=3.0cm]{geometry}
\usepackage{graphicx}
% \graphicspath{/Users/user/Documents/GitHub/kursach/Documents}
\usepackage{float}

\usepackage{amsfonts,amssymb,amscd,amsmath,amsthm}
\usepackage{mathrsfs}
\usepackage{epsf}
\usepackage{wrapfig}
\usepackage{cite}
\usepackage[hyper]{amsbib}
% \usepackage[backend=bibtex]{my_booksiki}
     
     
     


%% поля, заполняемые редакцией
\date{17 мая}\ydate{2019}%дата получения
\dateen{May 17}
\firstpage{1}%начальная страница статьи

%% поля, заполняемые автором
\title{Курсовая работа на тему: "Анализ тональности текста методами машинного обучения"}
\titlecr{Курсовая работа}
\author{Пехтерев Д.О.}


\authorcr{Пехтерев Д. О.} %Фамилия И.О. автора для строки с авторским знаком



%\UDC{517.9} %Номер УДК
%\DOI{10.18255/1818-1015-} % заполняется редакцией
\shorttitle{Анализ тональности текста методами машинного обучения}
\entitle{Sentiment Analysis by machine learning methods}%Название статьи на английском языке. Каждое значимое слово пишется с большой буквы
%Фамилия И.О. автора на английском языке:
\enauthorcr{Pekhterev D.O.}

%информация об авторах на русском языке: ФИО полностью, ученая степень, ученое звание, официальное название места работы или учебы в именительном падеже, официальный почтовый адрес, e-mail:
\authorinform{
\\Пехтерев Денис Олегович, студент направления: "Суперкомпьютерное моделирование в науке и инженерии" \\НИУ ВШЭ МИЭМ им. А.Н.Тихонова\\ ул. Мясницкая, 20, г.Москва,  Россия, 101000 \\e-mail: dopekhterev@edu.hse.ru
\vspace{4pt}	
}

%%%%Макросы необходимые автору
\def\W{\overset{\circ}{W}}
\newcommand{\ve}{\varepsilon}
\newcommand{\eps}{\varepsilon}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newtheorem{theor}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{dfn}{Определение}
%%%%%Макросы необходимые автору

\begin{document}
\maketitle

\begin{abstract}

	Было проведено исследование предыдущих работ для определения того, какие методы являются современными, какие их особенности уже были выявлены, а также для определения вектора развития данной области.
	В расчётной составляющей произведено изучение алгоритма по анализу тональности текстовых данных, где был рассмотрен и использован метод логистической регрессии. Для выделения признаков были рассмотрены три различных метода: Мешок слов (от англ. BoW), TF-IDF и N-граммы, в итоге был использован TF-IDF.
	Данные, используемые в работе были собраны пользователем Julian McAuley с сайта Amazon за период: с мая 1996 по июль 2014 года, где мною были выбраны различные категории товаров, такие как: Рецензии на книги, Здоровье и личная гигиена, Кино и Телевидение и Электроника.
	
\end{abstract}

\makeinfo

\normalsize


\section{Введение}



Мы живём в то время, когда интернет есть в каждом нашем устройстве, независимо от того, где мы находимся и чем занимаемся. Что касается России, то к началу 2019 года количество интернет-пользователей среди населения 16+ стало равно 90 миллионов человек, что есть 75,4\% взрослого населения страны. \cite{1_int_resource} 
Проникая во всё более новые отрасли нашей жизни, интернет стал наполняться огромным количеством информации, генерируемой потребителями продуктов. Стоит заметить, что почти у каждого конкурентноспособного бренда есть как минимум свой сайт или форум, поэтому клиенты имеют возможность расписать свой личный опыт использования того или иного продукта. Если смотреть на этот процесс со стороны успешного функционирования бизнеса, то одна из его важнейших частей – понимание клиента и его потребностей. Приблизительно 95\% потребителей в возрасте от 18 до 34 лет читают отзывы на продукт или услугу перед покупкой, из них 91\%  доверяют онлайн отзывам так же, как и личным рекомендациям. \cite{2_int_resource} 

\subsection{Поставновка цели и задачи}

Цель работы: применение современных средств прикладной математики для анализа данных

Задача работы: Анализ социальной активности потребителей в интернете, касающейся тональности оставленных ими отзывов

Основная задача: Анализ тональности отзывов методами машинного обучения

Подходы: Машинное обучение.

Используемый метод: Метод логистической регрессии

%\begin{itemize}
%  \item Метод логистической регрессии
%\end{itemize}


\subsection{Обсуждение постановки задачи}

Обычно перед компанией стоит задача по мониторингу реакции клиента на: 
\begin{itemize}

  \item Выпуски новых продуктов (запуск новых сервисов);
  \item Внесение изменений в существующие продукты (услуги);
  \item Внедрённые или готовящихся к внедрению инициатив, связанных с обслуживанием клиентов;
  \item Запуск новых рекламных компаний;

\end{itemize}


Наиболее же важным для компаний является определение того, какие их действия вызывают положительный отклик у клиентов, а какие отрицательный.

Зачастую, количество отзывов по конкретным продуктам (услугам), может превышать 10000 записей в день. Данная цифра включает в себя также и сообщения, поступающие в службу поддержки, которые необходимо сразу же сортировать и классифицировать, ведь от этого зависит опыт использования продукта клиентом и его мнение о том, на сколько дорог он как клиент для компании. 
Одной из задач, которая стоит перед компаниями сейчас – является задача по определению тональности текста (в пер. с англ. sentiment analysis). 

Сейчас решением данной задачи на Российском рынке занимаются два крупных сервиса: YouScan и Brand Analytics.
С одной стороны, эти системы кажутся абсолютно схожими – они решают одинаковые задачи и дают примерно одинаковый анализ. 
Главной же проблемой данных решений является их цена и ограниченный функционал. Рассмотрим ценообразование, которое используется сервисами YouScan и Brand Analytics по минимальной стоимости:
\begin{itemize}
  \item Standard , YouScan – 35 000 руб / мес
  \begin{itemize}
  	\item за каждые 5 тем в месяц
  	\item 100 000 упоминаний в каждой теме
  \end{itemize}
  \item Стартовый плюс , Brand Analytics – 35 000 руб / мес
  \begin{itemize}
  	\item сообщений в аккаунте за месяц: 20 000 сообщений
  	\item количество тем: 5
  	\item автоматическая разметка тематик (тегов на тему): 30\footnote{например, бывают тэги: головная боль, кожный зуд и т.д.} 
  \end{itemize}
  
\end{itemize}

Как мы видим, сервисы по своей сути предлагают одинаковый пул услуг: ограниченный набор тем (вы можете отслеживать только 5 услуг/товаров/запросов) и имеют ограничение на количество упоминаний (другими словами, за эти деньги, больше указанного выше лимита вы увидеть не сможете).
Большим и достаточно развитым бизнесам не составит труда купить подписку подороже и отслеживать свои товары на ежедневной основе, получая при этом достаточно хороший сервис.
Однако здесь оказывается незанятой другая ниша рынка – компании, имеющие достаточно много продуктов (брендов 50), но не выделяющих большие бюджеты на анализ отзывов, оставляемых относительно их продуктов. Такой анализ был бы полезен продакт-менеджерам \footnote{Люди, возглавляющие все активности, связанные с продуктом} для того, чтобы отслеживать реакцию потребителей и смотреть на то, как они воспринимают/потребляют продукт, и с какими сложностями они сталкиваются.

К таким компаниям относятся:
\begin{itemize}
  \item Фармацевтические компании;
  \item Медицинские клиники;
  \item Маркетплейсы\footnote{На англ. online marketplace, что в переводе на русский язык есть онлайн-магазин электронной торговли};
\end{itemize}


\section{Обзор литературы}


В этой части работы будут проанализированы статьи других авторов, напрямую связанные с анализом тональности текста, анализом текста, содержащегося в отзывах о покупках на различных сайтах и анализом тональности текста, содержащегося в социальных сетях, таких так Twitter.

В обзорной статье \cite{medhat2014sentiment} авторы изучили основные алгоритмы как по выделению признаков, так и по построению моделей, способных определять тональность текста. Говоря об анализе тональности, авторы обращаются к рисунку \ref{cat}, который описывает существующие и использующиеся сейчас техники по анализу тональности текста.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=450]{methods.jpg}
	\caption{Техники по анализу тональности текста (на основе оригинального рисунка 2 из статьи \cite{medhat2014sentiment} )}
	\label{cat} 
\end{figure}

Также авторы рассмотрели основные методы по извлечению признаков из текста:
\begin{enumerate}

\item Методы, в основе которых лежит измерение корреляции между словами и классами:

  \begin{itemize}
  
  	\item Метод поточечной взаимной информации (от англ. Point-wise Mutual Information (PMI)): метод основан на введении и использовании меры, которая показывает контекстную связь между признаками и классами. Пример использования: метод использовался в работе Yu и Wu \cite{yu_wu_pmi}, где авторы при помощи данного метода расширяли первоначальный набор слов, который был собран из небольшого набора текстов, посвященных новостям с фондового рынка. Результаты их модели превзошли результаты других разновидностей моделей PMI;
	
  	\item Хи-квадрат (от англ. Chi-square ($\chi^{2}$) ): метод также отвечает за поиск величины связи конкретного слова и класса, однако лучше PMI в том, что внутри него используются нормализованные значения, соответственно, эти значения больше подходят к терминам в той же категории. Пример использования: в результате исследования Фаном и Чангом \cite{fan_chia_blogger_cca} контекстной рекламы, ими было показано, что их метод может выделить рекламные объявления, которые непосредственно коррелируют с интересами блогера. Для выделения признаков в работе использовался метод хи-квадрат, тогда как для модели классификации использовался метод опорных векторов, изображённый на рисунке \ref{cat};
  \end{itemize}

\item Методы, суть которых заключается в уменьшении размерности данных:
	\begin{itemize}
		\item Латентно-семантический анализ (от англ. Latent Semantic Indexing (LSI))
	\end{itemize}
  
\item Методы, основанные на статистическом подходе:
	\begin{itemize}
		\item Hidden Markov Model (HMM)
		\item Latent Dirichlet Allocation (LDA)
	\end{itemize}

\item Другие методы:
	\begin{itemize}
		\item Метод Мешка слов (от англ. Bag of Words (BoW))
	\end{itemize}

\end{enumerate}

Авторы в \cite{gamal2019analysis} провели исследование по анализу тональности наборов данных, в состав которых входили: Cornell Movie Dataset, Twitter Dataset, Amazon Products Dataset и IMDB Dataset, используя различные методы машинного обучения.
В работе рассматривались следующие методы: Naive Bayes (NB), стохастический градиентный спуск (Stochastic gradient descent - SGD), метод опорных векторов (support vector machine - SVM), пассивный агрессивный метод (Passive Aggressive - PA), максимальной энтропии (Maximum Entropy), Adaptive Boosting (AdaBoost), Multinomial Na?ve Bayes (MNB), Bernoulli Naive Bayes (BNB), Ridge Regression (RR) и Logistic Regression (LR). В результате использования данных методов вместе с методами по извлечению признаков из наборов данных: TF-IDF и N-Грамм (юниграмма, биграмма и триграмма), было получено, что использование алгоритмов PA и RR по сравнению с остальными методами дают одни из самых высоких результатов классификации: от 87\% до 99.96\%, полученный при использовании метода PA. Также было выделено, что метод логистической регрессии и метод опорных векторов также давали приемлемые результаты: 87.56\% и 85.76\% соответственно.


Авторы в \cite{sa_pofonlrev_empty_r} проводят анализ тональности текста (положительный/отрицательный), используя набор данных, состоящий из отзывов, оставленных пользователями на сайте Amazon об электрических товарах (Kindle, DVD и других) в период с февраля 2012 года по июль 2017 года. Для извлечения признаков из данных использовался метод TF-IDF, а для обучения модели были рассмотрены методы машинного обучения: 3 разновидности Naive Bayes классификатора: обычный метод Naive Bayes, Multinomial Naive Bayes и Bernoulli Naive Bayes, а также логистическая регрессия. В итоге были получены следующие результаты: Multinomial Naive Bayes - 92.87\%, Bernoulli Naive Bayes - 92.35\% и Логистическая регрессия - 93.34\%.

Те же самые авторы в \cite{sa_ofofr_cr} изучали применение методов по анализу тональности текста, используя библиотеки языка программирования R по машинному обучению применительно к отзывам покупателей о приобретённой еде на сайте Amazon. В работе описан алгоритм при помощи которого проводилась классификация отзывов, а также приведены облаки слов, которые имели больший вес в модели при определении тональности.

Ещё одна работа по анализу тональности отзывов с сайта Amazon является работа \cite{bhatt2015amazon}, где авторы предлагают алгоритм по обработке отзывов, призванный разбить оставленные пользователями отзывы на две категории:
	\begin{itemize}
		\item Отзывы о представленном сервисе;
		\item Отзывы о характеристиках продукта;
	\end{itemize}

Далее авторами используется собственный алгоритм, который проводит анализ отзывов на предмет того, является ли отзыв прямым или косвенным. Например: “Камера не лучше, чем у iPhone 4s” - косвенный отзыв, фразы из предложений сравниваются с заранее выделенными в ходе ручной работы с отзывами фразами, по итогу чего и происходит классификация каждого отзыва. Также интересна обработка отрицаний в статье авторов, когда алгоритм встречает в отзыве фразу “не хороший товар”, то сначала алгоритм анализирует тональность слова хороший (стоит также отметить, что он ищет следующую связку: частицу, обозначающую отрицание и прилагательное). После чего, сначала, при помощи словаря тональности определяется тональность прилагательного и, так как ещё есть частица, означающая отрицание, то данной фразе проставляется противоположная тональность.

Алгоритм работы с отзывами, после извлечения набора фраза из предложений, может быть представлен следующим образом:
	\begin{itemize}
		\item Все фразы проверяются на то, являются ли они косвенными или прямыми относительно продукта;
		\item Проверяются на наличие с частицей “не”;
	\end{itemize}


\section{Этапы работы по определению тональности}

\subsection{Нормализация и токенизация текста}


Нормализация текста подразумевает приведение всего текста к одному виду, также можно разделить на следующие шаги:

\begin{itemize}
	\item Удаление знаков препинания
	\item Удаление лишних пробелов
	\item Приведение всех слов к нижнему регистру
\end{itemize}

Токенизация - разделение всех предложений в наборе данных на слова. В английском языке, также как и в русском это происходит при помощи разделения предложений по пробелам.

\subsection{Обработка слов в тексте}

Под обработкой слов в тексте понимается первоначальная обработка слов с целью их приведения к базовой форме. 

Для этого принято использовать два метода:
\begin{itemize}
	\item Лемматизация (на англ. Lemmatization)
	\item Стемминг (на англ. Stemming)
\end {itemize}

Целью двух алгоритмов является приведение слов к их первоначальной форме. Это делается для того, чтобы в дальнейшем при векторизации текста, одни и те же слова кодировались одинаково. 

Например:

\begin{itemize}
	\item Слова "на фрукте, фруктовый, у фруктов, фрукты" будут приведены к одинаковому виду: "фрукт"
	\item Предложение, как "я иду на работу с утра" будет приведено к виду: "я идти работа с утро"
\end {itemize}

Стемминг - процесс, заключающийся в отбрасывании от слова приставок и суффиксов с целью как можно точнее выделить основу слова. Стоит отметить, что такая основа слова не всегда будет совпадать с общепринятым корнем слова. 

Например, для русского языка, слово "играл"  будет приведено к виду: "игра", также как и для английского, слово: "played" будет приведено к виду "play".

Одним из самых знаменитых стеммеров\footnote{Конкретная реализация метода Стемминг} для английского языка является стеммер разработанный Мартином Портером\cite{porter1980algorithm}. Алгоритм состоит из 5 этапов обработки слов, которые применяются друг за другом. Суть алгоритма заключается в том, что при обработке каждого слова смотрится его суффикс, который сравнивается с заранее заданными в базе суффиксов: если находится совпадение, то суффикс отбрасывается. 

Преимуществом является то, что не нужно держать базу основ всех существующих в английском языке слов.

Недостатком же является то, что алгоритм может обрезать часть слова, которая несёт смысл, например: "кроватью" будет преобразовано в "крова".

Лемматизация - процесс по обработке слов в тексте, суть которого заключается в том, чтобы использовать словарные формы слов (леммы), удалив при этом лишние приставки и суффиксы. \cite{halacsy2006benefits} 

Отличием лемматизации от стемминга является то, что процесс лемматизации полностью выстроен на поиске начальной (словарной) формы слова, что может достигаться за счёт поиска части речи и других преобразований слов. 

Помимо широко использования вышеперечисленных алгоритмов в задачах по машинному обучению, они также применяются в большом количестве других областей:
\begin {itemize}
	\item Системы индексации в поисковых системах. Индексация в поисковых системах - процесс обработки сайта (сведений с сайта) и добавлением этих сведений роботом к себе в базу данных. Делается это с целью оптимизации поиска информации.
	\item SEO\footnote{Search Engine Optimization, поисковая оптимизация)} - поисковое продвижение сайта, которое включает в себя повышение соответствия страниц поисковому запросу, оптимизации содержания и т.д. 
	\item Поисковые системы - веб-сервисы, созданные для поиска информации в интернете.
\end {itemize}




\subsection{Векторизация отзывов}
При анализе тех или иных текстов методами машинного обучения, текстовые данные требуется перевести в числовой формат.
Для этого существует определённых набор методик, таких как:
	\begin{itemize}
		\item Метод "Мешок слов" (BoW, bag-of-words) \cite{mcclure2017tensorflow}
		
		Метод переводит набор данных в матричный формат, где каждая строка является отдельным предложением или отдельным отзывом, тогда как столбец отвечает за 1 слово в этом предложении/отзыве. Элементами же матрицы являются частоты с которыми данные слова встречаются в предложении. Всего же формируется количество признаков равное количеству уникальных слов в наборе данных.
		
		Рассмотрим, например, следующие предложения: "Эта была отличная затея" и "Эта затея стоила сил". Разбивая на слова, получаем: 'эта', 'была', 'отличная', 'затея' и 'эта', 'затея', 'стоила', 'сил'. 
		Уникальными слова здесь являются: 'эта', 'была', 'отличная', 'затея', 'стоила', 'сил' - количество признаков равно 6.
		
		Применяя метод "Мешок слов" получаем два вектора:
			\begin{itemize}
				\item $[1,1,1,1,0,0]$ - для предложения "Эта была отличная затея"
				\item $[1,0,0,1,1,1]$ - для предложения "Эта затея стоила сил"
			\end{itemize}
		
				
		\item Метод TF-IDF (TF — term frequency, IDF — inverse document frequency).
		
		Метод, суть которого состоит в оценке важности слова как в контексте рассматриваемого экземпляра документа, так и относительно других документов, находящихся в коллекции. Зачастую метод TF-IDF также используется для оценки схожести текстов \cite{huang2011text}
		
		Алгоритм помогает избавиться от предлогов, союзов и других частей речи, которые часто встречаются в любом тексте. Например, в русском языке: "это"\ ,"и"\ , "а"\ , "или"\ , а в английском языке: "is"\ , "a"\ , "the"\ и др.
		
		Частота слов (TF) вычисляется как отношение количества вхождений конкретного слова в документе к общему количеству слов в этом документе и записывается в следующем виде:
		
		$$tf(t, d) = \frac{n_t}{\sum_k(n_k)} \eqno(1)$$
		
		, где $n_t$ - количество вхождений слова $t$ в  документ $n$, $\sum_k(n_k)$ - общее число слов в документе $n$.
		
		Обратная частота документа (IDF) - отношение общего количества документов в коллекции к числу документов, где присутствует данное слово и записывается в виде:
		
		$${idf}(t) = \ln{\frac{1 + n}{1+{df}(t)}} + 1 \eqno(2)$$
		, где $n$ отвечает за общее число документов, входящих в набор данных, тогда как ${df}(t)$ - количество документов, которые содержат слово $t$.
		
		В конечном итоге, меру на которую данный метод опирается, можно записать следующей формулой \cite{scikit_tfidf_term_weighting}: 
		
		$$ {tf-idf(t,d)}={tf(t,d)} \times {idf(t)} \eqno(3)$$

Также в большинстве случаев, после того как выполнено преобразование TF-IDF принято полученные вектора нормализовать, используя Евклидову норму, имеющую вид:

$$v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +v{_2}^2 + \dots + v{_n}^2}} \eqno(4)$$
		
		Теперь рассмотрим пример применения данного метода на следующих предложениях:
		\begin {itemize}
			\item "Тебя знаю точно"\ 
			\item "Знаю ли тебя"\ 
			\item "Знаю точно себя"\ 
		\end {itemize}
		
		Словарь уникальных слов будет состоять из 5 слов и иметь следующий вид: 'знаю', 'ли', 'себя', 'тебя', 'точно'.
		
		Рассмотрим первое предложение: "Тебя знаю точно"\ . 
		
		Начнём со слова 'тебя'.
		
		$$n = 6; {df}(t)_{{term1}} = 2; \text{idf}(t)_{{term1}} =
				\ln \frac{1+n}{{1+df}(t)} + 1 = \ln(4/3)+1 = 1.2877 \eqno(5)$$
				
		После нормализации Евклидовой нормой, величина будет иметь вид:
		
		$$\frac {(\ln(4/3)+1)} {((\ln(1)+1)^2+(\ln(4/3)+1)^2+(\ln(4/3)+1)^2)^{(1/2)}} = 0.6198 \eqno(6)$$
		
		Итоговое значение слова 'тебя' будет равно значению слова 'точно', так как встречается два раза в наборе данных и является одним из трёх слов в каждом предложении.
		
		По аналогии, подставляя корректные значения в формулы выше, получаем следующие значения, которые удобно записываются в виде матрицы. При расположении слов в таком порядке: 'знаю', 'ли', 'себя', 'тебя', 'точно', для трёх предложений выше получается следующая матрица:
		
		\begin{bmatrix}
			0.48133417 &  0 & 0 &  0.61980538 & 0.61980538\\
			0.42544054 &  0.72033345 & 0 &  0.54783215 & 0\\
			0.42544054 &  0 & 0.72033345 &  0 & 0.54783215\\
		\end{bmatrix}
		
		
		\item N-граммы (N-grams).
		Метод, основанный на методе Мешка слов, однако отличается тем, что данный метод позволяет учесть порядок слов в предложении, что является полезным свойством, особенно при анализе таких предложений как: "Продукт плох, совсем не качественный" или "Продукт качественный, совсем не плохой"\cite{muller2016introduction}. Данные предложения будут иметь одинаковое числовое представление при их обработке методом Мешок слов. Для такого рода ситуаций подходит метод n-граммы, так как позволяет захватить контекст использования данного слова, учитывая не только количество отдельных слов (токенов), но и пар слов или словосочетаний состоящих из 3-ёх слов. 
	\end{itemize}

\subsection{Формальная постановка задачи обучения линейного классификатора}

Формальная постановка рассматриваемой задачи: $X$ - множество объектов, которые представляют из себя текстовые данные. ${x_{1},...,x_{l}}\subset X$ - обучающая выборка. Здесь будет рассматриваться двухклассовая задача, где $y_{j} \in (-1,1)$ - признак, который отвечает за тональность: положительный/отрицательный.

%Имеем обучающую выборку: "предложение - тональность"\ , которая может быть представлена в виде: $X^m = {(x_1, y_1),...,(x_m,y_m)}$


%В логистической регрессии строится линейный классификатор $a$ типа\cite{mach_learn_logist_regress}:
%
%$$a(x,w) = sign(\sum^{n}_{j=1} w_j*f_j(x) - w_0) = sign\langle x,w \rangle \eqno(7)$$,
%
%где 
%\begin{itemize}
%
%\item $w_j$ - вес $j$-го признака
%\item $w_0$ - порог принятия решения
%\item $w=(w_0, w_1, ..., w_n)$ - вектор весов
%\item $\langle x,w \rangle$ - скалярное произведение признаков объекта на вектор весов
%\end{itemize}
%
%Задачей является настройка вектора весов $w$ по заданной выборке $X^m$
%
%Логистическая регрессия отличается функцией потерь, которая служит для решения этой задачи и отвечает за минимизацию эмпирического риска, имея вид:
%
%$Q(w) = \sum_i_=_1 {ln(1+exp(-y_i*\langle x_i,w \rangle))} \rightarrow \underset{w}{min}$
%
%После того, как вектор $w$ найден, можно вычислить как классификацию для объекта $x$, так и оценить апостериорные вероятности принадлежности объекта определённому классу:
%
%$P(y|x) = \sigma (y \langle x,w \rangle), y \in Y$, где:
%\begin{itemize}
%\item $\sigma(z) = 1/(1+e^(-x)$ - сигмоидная функция
%\end{itemize}
%

\subsection{Логистическая регрессия}
Для решения данной задачи бинарной классификации, будем использовать логистическую регрессию, которая является одним из методов построения линейного классификатора и даёт оценивать апостериорные вероятности принадлежности объектов классам. 

Логистическая регрессия строит регрессионную модель, которая может предсказывать вероятности того, что запись относится к классу 1. Как линейная регрессия использует линейную функцию, так логистическая регрессия основана на функции вида сигмоида, которая записывается следующей формулой:

$$\sigma(z) = \frac{1}{(1+e^{(-x)}} \eqno(7)$$

И имеет вид:

\begin{figure}[h]
  \center \includegraphics[width=350]{sigmoid_func.png}
  \caption{Логистическая функция (сигмоида).}
  \label{fig:sigmoid_func}
\end{figure}

Моделью классификации логистическая регрессия становится только после введения порогового значения. На самом деле, подбор правильного порогового значения зависит от значений метрик точность и полнота.

В зависимости от количества категорий, которые необходимо классифицировать, логистическая регрессия может иметь вид:

\begin{itemize}
	\item Биноминальная: целевая переменная принимает только два значения, например: "живой-мёртвый"\ , "да-нет"\ , "выиграет-проиграет"\ .
	\item Мультиномиальная: целевая переменная принимает больше двух значений, например: категории товаров - "категория А"\ ,"категория Б"\ ,"категория В"\ 
	\item Порядковая: целевая переменная принимает значения, принадлежащие порядковой шкале, например:  "неудовлетворительно"\ , "удовлетворительно"\ ,"хорошо"\ ,"отлично"\ .
\end{itemize}

Мною будет рассмотрена именно биноминальная логистическая регрессия. 
Задача, решаемая данным типом логистической регресии, может быть записана в виде:

 $$y = \begin{equation*}
 \begin{cases}
   1, & \text{положительная тональность} \\
   -1, &\text{отрицательная тональность}
 \end{cases}
\end{equation*} $$

Имеем обучающую выборку вида: $X^{m}=(x_i,y_i)^{m}_{i=1}$, где $x_i \in R^n$. В логистической регрессии также используется линейный алгоритм классификации, который имеет вид: $$a(x,w) = sign(\sum^{n}_{j=1} w_j*f_j(x) - w_0) = sign\langle x,w \rangle \eqno(8)$$
, где:
\begin{itemize}

	\item $w_j$ - вес $j$-го признака
	\item $w_0$ - порог принятия решения
	\item $w=(w_0, w_1, ..., w_n)$ - вектор весов
	\item $\langle x,w \rangle$ - скалярное произведение имеющихся признаков на вектор весов
\end{itemize}

Заметим, что на данном этапе искусственно введён нулевой признак: $f_0(x) = -1$.

Задачей обучения является настройка весов по имеющимся данным. В логистической регрессии для этой цели решается задача минимизации эмпирического риска, который имеет следующую функцию потерь:

$$Q(w) = \sum_{i=1}^{m} {ln(1+exp(-y_i*\langle x_i,w \rangle))} \rightarrow \underset{w}{min} \eqno(9)$$

После того, как вектор весов $w$ найден, модель готова к вычислению классификации для конкретного объекта $x$. Кроме того, для объекта $x$ возможно оценивать вероятности принадлежности этого класса к конкретным классам, используя формулу вида:

$$ P(y|x) = \frac {1}{1+e^{(-y*\langle x,w \rangle)}} \eqno(10)$$

Для решения данной задачи я использовал пакет sklearn, в котором содержится функция LogisticRegression. 

Мною использовалась штрафная функция типа $L2$, при том, что в пакете существует три варианта регуляризации:
\begin{itemize}
	\item L1 регуляризация имеет эффект отбора признаков, так как обнуляет веса неинформативных признаков:
	
	$$\min_{w, c} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1). \eqno(11)$$
	
	
	\item L2 регуляризация отвечает за решение проблему мультиколлиеарности, сокращает веса линейно зависимых признаков:
	
	$$\min_{w, c} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1). \eqno(12)$$
	
	\item Elastic-Net регуляризация является комбинацией двух регуляризаций (L1 и L2):
	
	$$\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) \eqno(13)$$
	

	, где коэффициент $p$ отвечает за силу $l_1$ регуляризации и $l_2$ регуляризации.


%Note that, in this notation, it’s assumed that the target  takes values in the set  at trial . We can also see that Elastic-Net is equivalent to  when  and equivalent to  when .

\end{itemize}

%Решающие функции, реализованные в LogisticRegression подразделяются на типы:


\subsection{Оценка точности классификации}
Исследователями выделяется большое количество оценки точности классификации.

К ним относятся:
\begin{itemize}
	
	\item Base{Rate} - базовая доля правильных ответов. Показатель показывает минимум, который должна превышать метрика Accuracy.
	
	$$Base{Rate}=\text{argmax }\frac{1}{l}\sum_{i=1}^{\text{l}}[y_{o}=y_{i}] \eqno(14)$$
	, где:
	\begin{itemize}
		\item $y_{o}$ - заданные объекты класса 
		\item $y_{i}$ - результат работы алгоритма
	\end{itemize}
	\item Accuracy - суть показателя заключается в том, что при исследовании точности модели мы смотрим на долю правильных ответов, которая может быть записана в следующем виде:

	$$Accuracy=\frac{TP+TN}{TP+FP+FN+TN} \eqno(15)$$
	, где:
	\begin{itemize}
		\item TP (True Positive) - количество истинно-положительных результатов, когда заданные объекты класса 1 равны результату алгоритма 1
		\item TN (True Negative) - количество истинно-отрицательных результатов, когда заданные объекты класса 0 равны результату алгоритма 0
		\item FP (False Positive) - количество ложно-положительных результатов, заданные объекты класса 0 равны результату алгоритма 1
		\item FN (False Negative) - количество ложно-отрицательных результатов, заданные объекты класса 1 равны результату алгоритма 0
	\end{itemize}

	\item Precision (точность алгоритма в пределах рассматриваемого класса) – это доля объектов действительно принадлежащих рассматриваемому классу относительно всех объектов которые алгоритм отнесла к этому классу. 
	
	Метрика Precision выражается следующей формулой:
	
	$$Precision=\frac{TP}{TP+FP} \eqno(16)$$
	\item Recall (полнота алгоритма) – это доля найденных классификатором объектов, принадлежащих определённому классу относительно всех объектов этого класса.	
	
	Метрика Recall выражается следующей формулой:
	
	$$ Recall=\frac{TP}{TP+FN} \eqno(17)$$
	
	Как видно из формул выше, точность и полнота характеризует разные стороны определения точности классификатора, именно поэтому на эти метрики принято смотреть в тандеме.
	
	\item Матрица ошибок (confusion matrix, матрица неточностей) - представляет из себя матрицу размером M на M, где M - отвечает за количество классов. В случае бинарной классификации матрица имеет размер 2 на 2, однако такие матрицы распространено стоить, если количество классов > 2. Столбцы матрицы относятся к истинным (заданным) ответам, строки же к результатам алгоритма. Отсюда следует, что чем большие числа стоят на диагоналях данной матрицы, тем лучше работает классификатор. 
	
	
	\item F-мера  – метрика, представляющая гармоническое среднее между точностью и полнотой. Другими словами, она объединяет два показателя в один, давая исследователю общее представление о качестве работы классификатора.
	Существует два вида F-меры:
	\begin{itemize}
		\item Если двум метрикам - полноте и точности отдаётся одинаковое предпочтение, то есть максимизируются оба этих значения, то F-мера называется сбалансированной и принимает следующий вид:
		
		$$F = 2 \frac{Precision \times Recall}{Precision + Recall} \eqno(18)$$
		
		\item Также есть возможность придать различный вес точности и полноте, тогда F-мера принимает следующий вид:
		
		$$F = \left(\alpha^2+1\right)\frac{Precision \times Recall}{\alpha^2 Precision + Recall} \eqno(19)$$
	, где:  если мы хотим улучшить метрику точности, то нужно задавать  $\alpha$ в диапазоне $0<\alpha<1$, если же мы хотим улучшить метрику полноты, то нужно задавать  $\alpha$ в диапазоне $1<\alpha$. 
	\end{itemize}
	
	Используя сбалансированную F-меру удобно быстро принимать решения о качестве алгоритма, так как здесь учитываются две основные метрики, используемые при бинарной классификации: полнота и точность.
	
	
	\item ROC – кривая (Receiver Operating Characteristic, иногда называют кривой ошибок) - служит для визуализации результата работы алгоритма, тогда как для определения качества используют площадь под кривой AUC (Area Under the Curve). 
	
	Для этого понадобятся следующие формулы:
	
	\begin{itemize}
		
		\item FPR - доля ложно-положительных результатов относительно количества объектов, принадлежащих нулевому классу. Откладывается по оси X.
			$$FPR=\frac{FP}{FP+TN}$$
		
		
		
		\item TPR - доля истинно-положительных результатов относительно количества объектов, принадлежащих первому классу.  Откладывается по оси Y. 
			$$TPR=\frac{TP}{TP+FN}$$	
		
	\end{itemize}
	
	
\end{itemize}





\section{Описание наборов данных}

В данной работе использовался следующий набор данных: отзывы на категории товаров, представленные в разных категориях на сайте Amazon в период с мая 1996 по июль 2014 года. Набор данных был собран в файлы по категориям профессором Стэнфордского университета Джулианом Макаули. \cite{cite_amazon_rewies} 

Были взяты наборы данных по следующим категориям:

\begin{enumerate}
	
	\item Отзывы на товары категории "Рецензии на книги"
	
	Всего отзывов в категории: 8\,898\,041 записей
	\item Отзывы на товары категории "Электроника"
	
	Всего отзывов в категории: 1\,689\,188
	\item Отзывы на товары категории "Кино и Телевидение"
	
	Всего отзывов в категории: 1\,697\,533
	\item Отзывы на товары категории "Компакт-диски и винилы"
	
	Всего отзывов в категории: 1\,097\,592
	\item Отзывы на товары категории "Kindle Store"
	
	Всего отзывов в категории: 982\,619


\end {enumerate}

Наборы данных были представлены в одинаковом формате JSON\footnote{JavaScript Object Notation - текстовый формат обмена данными, основанный на JavaScript.} и состояли из следующих колонок:

	\begin{itemize}
		\item reviewerID – уникальный идентификатор пользователя
		\item asin – уникальный идентификатор продукта
		\item reviewerName – имя человека, оставившего отзыв
		\item helpful – оценка полезности отзыва
		\item reviewText – текст отзыва
		\item overall – рейтинг, оставленный пользователем (от 1 до 5)
		\item summary – краткий итог отзыва
		\item unixReviewTime/reviewTime – время, когда отзыв был оставлен пользователем
	\end{itemize}
	
Из этих столбцов для решаемой задачи будут интересны только reviewText - сам отзыв, оставленный посетителем сайта и overall - оценка пользователя, которая ставится в соответствии с его отзывом.

Итак, обработав отзывы в формате json и переведя их в формат pandas\footnote{Pandas - библиотека языка Python для работы с наборами данных} - DataFrame, мы получили 5 наборов данных. Для начала будем рассматривать в каждом из этих наборов данных максимальное количество записей - 1 млн.

\subsection{Обработка данных}

Первым шагом в данном упражнении, переведём оценки из формата 1-5 в 2 класса, которые понадобятся нам для классификации (-1/1), для этого воспользуемся следующей логикой:

	\begin{itemize}
		\item при overall $\in$ (1,2) присвоим значение -1, что будет означать, что отзывы данной категории имеют отрицательную тональность;
		\item при overall $\in$ (3)  не будем присваивать никакого значения, так как мы будем рассматривать задачу бинарной классификации;
		\item при overall $\in$ (4,5) присвоим значение 1, так как оставляя отзывы с такими оценками, мы можем быть уверены, что пользователь был удовлетворён своей покупкой, а значит отзыв имеет положительную тональность;
	\end{itemize}
	
Имеем 5 наборов данных, состоящих из отзывов пользователей и их оценок, где в каждом наборе данных содержится приблизительно миллион записей. Так как в задачах машинного обучения важно иметь сбалансированные выборки, а в разных категориях товаров разное количество положительных и отрицательных отзывов, то была написана функция, которая считает в каждом наборе данных количество положительных и отрицательных отзывов, берёт наименьшее из них и обрезает класс, где отзывов больше, тем самым получается равное количество положительных и отрицательных отзывов в выборке.
	
\caption{Количество отзывов по классам тональности }

$\begin{tabular}{c|f|f|ff|} 

Название категории & Положительные & Нейтральные & Отрицательные \\ 
\hline 
\hline 
"Рецензии на книги" &  7\,203\,909	&   955\,189	& 738\,943	\\ 
"Кино и Телевидение" & 1\,289\,602	& 201\,302 & 206\,629	\\ 
"Электроника" & 1\,356\,067  & 	142\,257  & 190\,864 	\\ 

"Компакт-диски и винилы" & 903\,002  & 	101\,824  & 92\,766 	\\ 
"Kindle Store" & 829\,277  & 	96\,194  & 57\,148 	\\ 

\hline 
\end{tabular}$

\section{Проведение эксперимента}

\subsection{Подбор коэффициента регуляризации C}

Подбор коэффициента регуляризации C осуществлялся путём определения качества модели на тестовой выборке по метрике Accuracy.

\subsection{Результаты}
Результаты работы классификатора:


$\begin{tabular}{c||rrrr|} 
Название категории & Accuracy Rate, \% \\ 
\hline 
\hline 
"Рецензии на книги" & 91.47\% \\ 
"Здоровье и личная гигиена" & 85.78\%  \\ 
"Кино и Телевидение" & 90.65\% \\ 
"Электроника" & 89.84\% \\ 
\hline 
\end{tabular}$

Здесь мы можем увидеть результаты работы классификатора, реализованного с использованием логистической регрессии. Так как отзывов по категории товаров: "Здоровье и личная гигиена" было меньше всего, то можем наблюдать не такие высокие результаты работы классификатора, как по остальным категориям.


Интересно изучить слова, которые оказали наибольшее влияние на работу классификатора. Так, например, при анализе отзывов на категорию книги одним из важных факторов для модели при определении тональности стало наличие в предложении автора: Joseph Klausner. Это связано с тем, что под произведениями данного автора, представленными на сайта Amazon, стояло больше всего положительных (как мы помним, оценки 4,5) отзывов, что свидетельствует о любви читателей к определённому автору в рамках рассматриваемого набора данных.




\subsection{Анализ результатов}
%Анализ результатов

После опытов следует анализ полученных результатов (графиков, таблиц и др.), обсуждение полученного, выводы.

\section{Заключение}

Заключение должно содержать основные результаты работы, сделанные вами выводы и 
выдвинутые гипотезы.
--------------

Цель работы: применение современных средств прикладной математики для анализа данных

Задача работы: Анализ социальной активности потребителей в интернете, касающейся тональности оставленных ими отзывов

Основная задача: Анализ тональности отзывов методами машинного обучения

Подходы: Машинное обучение.

Используемый метод: Метод логистической регрессии

Анализ социальной активности потребителей в интернете, касающейся тональности оставленных ими отзывов является актуальной задачей, что видно по росту количества научных работ, посвященных этой теме. Кроме этого, в этом есть и прямая бизнес потребность, так как каждая компания хочет знать, что об её продуктах/услугах думают потребители.

В работе было проведено исследование предыдущих работ для определения того, какие методы являются современными, какие их особенности уже были выявлены, а также для определения вектора развития данной области.
	Далее были детально рассмотрены основные этапы работы по определению тональности:
	\begin{itemize}
		\item Нормализация и токенизация текста
		\item Обработка слов в тексте
		\item Векторизация отзывов
		\item Логистическая регрессия
		\item Оценка точности классификации		
	\end{itemize} 
	
Данные, используемые в работе были собраны пользователем Julian McAuley с сайта Amazon за период: с мая 1996 по июль 2014 года, где были выбраны различные категории товаров, такие как: Рецензии на книги, Здоровье и личная гигиена, Кино и Телевидение и Электроника.
		
	В расчётной составляющей произведено изучение алгоритма по анализу тональности текстовых данных, где был рассмотрен и использован метод логистической регрессии. При этом был написан алгоритм, который из заранее заданного диапазона подбирает и использует для классификации параметр регуляризации модели C. Исходя из графика видно, что для разных наборов данных оптимальным являлся параметр С = . Используя это наблюдение, можем сделать вывод, что вводя такую модель в полноценную систему, отвечающую за классификацию отзывов, можно сразу поставить параметр С =, для того чтобы сократить время на его подбор. 
	Кроме этого были визуализированы результаты работы логистической регрессии, связанные с весами коэффициентов: было построено облако слов, анализируя которое можно выявить какие слова ассоциируются с положительными эмоциями у потребителей, а какие с отрицательными. Из-за простой формы подачи материалы, таким анализом смогут пользоваться люди, не имеющих специальных знаний в машинному обучении, что является положительным фактором внедрения такой модели в компании.
	 






\medskip
\renewcommand{\refname}{Список литературы / References}


{\footnotesize

\begin{thebibliography}{99}

\bibliography{my_booksiki}

\RBibitem{1_int_resource}
\by Сергей Орлов
\paper Журнал о современных тенологиях
\jour КОМПЬЮТЕРРА
\yr 16 января 2019
% \pages 27

\RBibitem{2_int_resource}
\by Rosie Murphy
\paper Local consumer review survey 2018
\jour Веб-сайт
\yr 7 декабря 2018
% \pages 27


\Bibitem {medhat2014sentiment}
\by Medhat, Walaa and Hassan, Ahmed and Korashy, Hoda
\paper Sentiment analysis algorithms and applications: A survey
\jour Ain Shams engineering journal
\yr 2014
\vol 5
\pages 1093--1113


\Bibitem {yu_wu_pmi}
\by Yu Liang-Chih, Wu Jheng-Long, Chang Pei-Chann, Chu Hsuan-Shou
\paper Using a contextual entropy model to expand emotion
words and their intensity for the sentiment classification of stock
market news
\jour Knowl-Based Syst
\yr 2013
\vol 41
\pages 89–97


\Bibitem {fan_chia_blogger_cca}
\by Teng-Kai Fan, Chia-Hui Chang
\paper Blogger-Centric Contextual Advertising
\jour Expert Systems with Applications
\yr 2011
\vol 38
\issue 3
\pages 1777-1788




\Bibitem {gamal2019analysis}
\by Gamal, Donia and Alfonse, Marco and M El-Horbaty, El-Sayed and M Salem, Abdel-Badeeh
\paper Analysis of Machine Learning Algorithms for Opinion Mining in Different Domains
\jour Machine Learning and Knowledge Extraction
\publ Multidisciplinary Digital Publishing Institute
\yr 2019
\vol 1
\pages 224--234

\Bibitem {sa_pofonlrev_empty_r}
\by Sasikala P, L.Mary Immaculate Sheela
\paper Sentiment Analysis and Prediction of Online Reviews with Empty Ratings
\jour International Journal of Applied Engineering Research
\publ Research India Publications
\yr 2018
\vol 13
\pages 11525--11531


\Bibitem {sa_ofofr_cr}
\by Sasikala P, L.Mary Immaculate Sheela
\paper Sentiment Analysis of Online Food Reviews using
Customer Ratings
\jour International Journal of Pure and Applied Mathematics
\publ 
\yr 2018
\vol 119
\pages 3509--3514

\Bibitem {bhatt2015amazon}
\by Bhatt, Aashutosh and Patel, Ankit and Chheda, Harsh and Gawande, Kiran
\paper Amazon Review Classification and Sentiment Analysis
\jour International Journal of Computer Science and Information Technologies
\publ Citeseer
\yr 2015
\vol 6
\pages 5107--5110

\Bibitem {mcclure2017tensorflow}
\by McClure, Nick
\paper TensorFlow machine learning cookbook
\publ Packt Publishing Ltd
\yr 2017
\pages 185--187

\Bibitem {porter1980algorithm}
\by Porter, Martin F
\paper An algorithm for suffix stripping
\jour Program
\publ MCB UP Ltd
\yr 1980
\vol 3
\pages 130--137


\Bibitem {halacsy2006benefits}
\by Hal{\'a}csy, P{\'e}ter and Tr{\'o}n, V
\paper Benefits of deep NLP-based Lemmatization for Information Retrieval.
\jour CLEF (Working Notes)
\publ MCB UP Ltd
\yr 2006


\Bibitem {huang2011text}
\by Huang, Cheng-Hui and Yin, Jian and Hou, Fang
\paper A text similarity measurement combining word semantic information with TF-IDF method
\jour Jisuanji Xuebao(Chinese Journal of Computers)
\publ Science Press(Beijing),| a 16 Donghuangchenggen North Street| c Beijing| z~…
\yr 2011
\vol 34
\pages 856--864


\Bibitem {scikit_tfidf_term_weighting}
\by https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting





\Bibitem {muller2016introduction}
\by M{\"u}ller, Andreas C and Guido, Sarah and others
\paper Introduction to machine learning with Python: a guide for data scientists
\publ  O'Reilly Media, Inc.
\yr 2016
\pages 339--341


\Bibitem {cite_amazon_rewies}
\by узнать как сайты оформлять




\Bibitem {mach_learn_logist_regress}
\by узнать как сайты оформлять
http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F


%\RBibitem{1}
%\by Мальцев~А.\,И.
%\paper Об изоморфном представлении бесконечных групп матрицами
%\jour Мат. сб.
%\yr 1940
%\vol 8
%\issue 3
%\pages 405--422
%\transl
%[\by Malcev~A.\,I.
%\paper Ob izomorfnom predstavlenii beskonechnykh grupp matritsami
%\jour Mat. sb.
%\yr 1940
%\vol 8
%\issue 3
%\pages 405--422
%\finalinfo (in Russian).\nofrills]
%
%\Bibitem{2}
%\by Hirsh~K.\,A.
%\paper On infinite soluble groups
%\jour J. London Math. Soc.
%\yr 1952
%\vol 27
%\pages 81--85
%
%\Bibitem{L}
%\by Learner~A.
%\paper Residual properties of polycyclic groups
%\jour J. Math.
%\yr 1964
%\vol 8
%\pages 536--542
%
%\RBibitem{Se}
%\by Сексенбаев~К.
%\paper К теории полициклических групп
%\jour Алгебра и логика
%\yr 1965
%\vol 4
%\issue 3
%\pages 79--83
%\rtransl
%[\by Seksenbaev~K.
%\paper K teorii policiklicheskih grupp
%\jour Algebra i logika
%\yr 1965
%\vol 4
%\issue 3
%\pages 79--83
%\finalinfo (in Russian).\nofrills]
%
%\RBibitem{3}
%\by Шмелькин~А.\,И.
%\paper Полициклические группы
%\jour Сиб. мат. ж.
%\yr 1968
%\vol  9
%\pages 234--235
%\transl
%[\by Smelkin~A.\,L.
%\paper Politsiklicheskie gruppy
%\jour Sib. mat. zh.
%\yr 1968
%\vol  9
%\pages 234--235
%\finalinfo (in Russian).\nofrills]
%
%\Bibitem{4}
%\by Gruenberg~K.\,W.
%\paper Residual properties of infinite soluble groups
%\jour Proc. London Math. Soc.
%\yr 1957
%\vol 3
%\issue 7
%\issue 25
%\pages 29--62
%
%
%\bibitem{} 



\end{thebibliography}
}
\vskip 8pt
   \smallskip\hrule width 6cm
\medskip

\end{document}





%Backup
%Тогда векторное описание объекта имеет следующий вид:

%$F=||f_{j}(x_{i})||_{l*n}= \left(\begin{array}{cccc} f_{1}(x_1) & \ldots & f_{n}(x_1)\\ \vdots & \ddots & \vdots\\ f_{1}(x_l) & \ldots & f_{n}(x_l) \end{array} \right)$


