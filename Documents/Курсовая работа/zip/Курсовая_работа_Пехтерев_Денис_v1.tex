\documentclass{mais}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[mag=1000,a4paper,left=2.2cm,right=3.0cm,top=3.1cm,bottom=3.0cm]{geometry}
\usepackage{graphicx}
% \graphicspath{/Users/user/Documents/GitHub/kursach/Documents}
\usepackage{float}

\usepackage{amsfonts,amssymb,amscd,amsmath,amsthm}
\usepackage{mathrsfs}
\usepackage{epsf}
\usepackage{wrapfig}
\usepackage{cite}
\usepackage[hyper]{amsbib}
% \usepackage[backend=bibtex]{my_booksiki}
     
     
     


%% поля, заполняемые редакцией
\date{17 мая}\ydate{2019}%дата получения
\dateen{May 17}
\firstpage{1}%начальная страница статьи

%% поля, заполняемые автором
\title{Курсовая работа на тему: "Анализ тональности текста методами машинного обучения"}
\titlecr{Курсовая работа}
\author{Пехтерев Д.О.}


\authorcr{Пехтерев Д. О.} %Фамилия И.О. автора для строки с авторским знаком



%\UDC{517.9} %Номер УДК
%\DOI{10.18255/1818-1015-} % заполняется редакцией
\shorttitle{Анализ тональности текста методами машинного обучения}
\entitle{Sentiment Analysis by machine learning methods}%Название статьи на английском языке. Каждое значимое слово пишется с большой буквы
%Фамилия И.О. автора на английском языке:
\enauthorcr{Pekhterev D.O.}

%информация об авторах на русском языке: ФИО полностью, ученая степень, ученое звание, официальное название места работы или учебы в именительном падеже, официальный почтовый адрес, e-mail:
\authorinform{
\\Пехтерев Денис Олегович, студент направления: "Суперкомпьютерное моделирование в науке и инженерии" \\НИУ ВШЭ МИЭМ им. А.Н.Тихонова\\ ул. Мясницкая, 20, г.Москва,  Россия, 101000 \\e-mail: dopekhterev@edu.hse.ru
\vspace{4pt}	
}

%%%%Макросы необходимые автору
\def\W{\overset{\circ}{W}}
\newcommand{\ve}{\varepsilon}
\newcommand{\eps}{\varepsilon}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newtheorem{theor}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{dfn}{Определение}
%%%%%Макросы необходимые автору

\begin{document}
\maketitle

\begin{abstract}

	Было проведено исследование предыдущих работ для определения того, какие методы являются современными, какие их особенности уже были выявлены, а также для определения вектора развития данной области.
	В расчётной составляющей произведено изучение алгоритма по анализу тональности текстовых данных, где используется метод логистической регрессии. Для выделения признаков были рассмотрены три различных метода: Мешок слов (от англ. BoW), TF-IDF и N-граммы, в итоге же использовался только TF-IDF.
	Данные, используемые в работе были собраны пользователем Julian McAuley с сайта Amazon за период: с мая 1996 по июль 2014 года, где мною были выбраны различные категории товаров, такие как: Рецензии на книги, Здоровье и личная гигиена, Кино и Телевидение и Электроника.
	
\end{abstract}

\makeinfo

\normalsize


\section*{Введение}

\subsection{Актуальность работы}

Мы живём в то время, когда интернет есть в каждом нашем устройстве, независимо от того, где мы находимся и чем занимаемся. Что касается России, то к началу 2019 года количество интернет-пользователей среди населения 16+ стало равно 90 миллионов человек, что есть 75,4\% взрослого населения страны. \cite{1_int_resource} 
Проникая во всё более новые отрасли нашей жизни, интернет стал наполняться огромным количеством информации, генерируемой потребителями продуктов. Стоит заметить, что почти у каждого конкурентноспособного бренда есть как минимум свой сайт или форум, поэтому клиенты имеют возможность расписать свой личный опыт использования того или иного продукта. Если смотреть на этот процесс со стороны успешного функционирования бизнеса, то одна из его важнейших частей – понимание клиента и его потребностей. Приблизительно 95\% потребителей в возрасте от 18 до 34 лет читают отзывы на продукт или услугу перед покупкой, из них 91\%  доверяют онлайн отзывам так же, как и личным рекомендациям. \cite{2_int_resource} 


Обычно перед компанией стоит задача по мониторингу реакции клиента на: 
\begin{itemize}

  \item Выпуски новых продуктов (запуск новых сервисов);
  \item Внесение изменений в существующие продукты (услуги);
  \item Внедрённые или готовящихся к внедрению инициатив, связанных с обслуживанием клиентов;
  \item Запуск новых рекламных компаний;

\end{itemize}

Наиболее же важным для компаний является определение того, какие их действия вызывают положительный отклик у клиентов, а какие отрицательный.

Зачастую, количество отзывов по конкретным продуктам (услугам), может превышать 10000 записей в день. Данная цифра включает в себя также и сообщения, поступающие в службу поддержки, которые необходимо сразу же сортировать и классифицировать, ведь от этого зависит опыт использования продукта клиентом и его мнение о том, на сколько дорог он как клиент для компании. 
Одной из задач, которая стоит перед компаниями сейчас – является задача по определению тональности текста (в пер. с англ. sentiment analysis). 

Сейчас решением данной задачи на Российском рынке занимаются два крупных сервиса: YouScan и Brand Analytics.
С одной стороны, эти системы кажутся абсолютно схожими – они решают одинаковые задачи и дают примерно одинаковый анализ. 
Главной же проблемой данных решений является их цена и ограниченный функционал. Рассмотрим ценообразование, которое используется сервисами YouScan и Brand Analytics по минимальной стоимости:
\begin{itemize}
  \item Standard , YouScan – 35 000 руб / мес
  \begin{itemize}
  	\item за каждые 5 тем в месяц
  	\item 100 000 упоминаний в каждой теме
  \end{itemize}
  \item Стартовый плюс , Brand Analytics – 35 000 руб / мес
  \begin{itemize}
  	\item сообщений в аккаунте за месяц: 20 000 сообщений
  	\item количество тем: 5
  	\item автоматическая разметка тематик (тегов на тему): 30\footnote{например, бывают тэги: головная боль, кожный зуд и т.д.} 
  \end{itemize}
  
\end{itemize}

Как мы видим, сервисы по своей сути предлагают одинаковый пул услуг: ограниченный набор тем (вы можете отслеживать только 5 услуг/товаров/запросов) и имеют ограничение на количество упоминаний (другими словами, за эти деньги, больше указанного выше лимита вы увидеть не сможете).
Большим и достаточно развитым бизнесам не составит труда купить подписку подороже и отслеживать свои товары на ежедневной основе, получая при этом достаточно хороший сервис.
Однако здесь оказывается незанятой другая ниша рынка – компании, имеющие достаточно много продуктов (брендов 50), но не выделяющих большие бюджеты на анализ отзывов, оставляемых относительно их продуктов. Такой анализ был бы полезен продакт-менеджерам \footnote{Люди, возглавляющие все активности, связанные с продуктом} для того, чтобы отслеживать реакцию потребителей и смотреть на то, как они воспринимают/потребляют продукт, и с какими сложностями они сталкиваются.
К таким компаниям относятся:
\begin{itemize}
  \item Фармацевтические компании;
  \item Медицинские клиники;
  \item Агрегаторы такси;
\end{itemize}

\subsection{Поставновка цели и задачи}

Цель работы: применение современных средств прикладной математики для анализа данных

Задача работы: Анализ социальной активности потребителей в интернете, касающейся тональности оставленных ими отзывов

Основная задача: Анализ тональности отзывов методами машинного обучения

Подходы: Машинное обучение.

Используемые методы:

\begin{itemize}
  \item Метод логистической регрессии
  \item Метод опорных векторов
  \item Метод гребневой регрессии
\end{itemize}


%\begin{equation}\label{eq1.1}
%\frac{\partial u}{\partial t}=\nu D\Delta u+F(u), \quad
%\frac{\partial u}{\partial\vec{n}}\bigg|_{\partial\Omega}=0,
%\end{equation}
%


\section{Обзор литературы}


В этой части работы будут проанализированы статьи других авторов, напрямую связанные с анализом тональности текста, анализом текста, содержащегося в отзывах о покупках на различных сайтах и анализом тональности текста, содержащегося в социальных сетях, таких так Twitter.

В обзорной статье \cite{medhat2014sentiment} авторы изучили основные алгоритмы как по выделению признаков, так и по построению моделей, способных определять тональность текста. Говоря об анализе тональности, авторы обращаются к рисунку \ref{cat}, который описывает существующие и использующиеся сейчас техники по анализу тональности текста.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=450]{methods.jpg}
	\caption{Техники по анализу тональности текста (на основе оригинального рисунка 2 из статьи \cite{medhat2014sentiment} )}
	\label{cat} 
\end{figure}

Также авторы рассмотрели основные методы по извлечению признаков из текста:
\begin{enumerate}

\item Методы, в основе которых лежит измерение корреляции между словами и классами:

  \begin{itemize}
  
  	\item Метод поточечной взаимной информации (от англ. Point-wise Mutual Information (PMI)): метод основан на введении и использовании меры, которая показывает контекстную связь между признаками и классами. Пример использования: метод использовался в работе Yu и Wu \cite{yu_wu_pmi}, где авторы при помощи данного метода расширяли первоначальный набор слов, который был собран из небольшого набора текстов, посвященных новостям с фондового рынка. Результаты их модели превзошли результаты других разновидностей моделей PMI;
	
  	\item Хи-квадрат (от англ. Chi-square ($\chi^{2}$) ): метод также отвечает за поиск величины связи конкретного слова и класса, однако лучше PMI в том, что внутри него используются нормализованные значения, соответственно, эти значения больше подходят к терминам в той же категории. Пример использования: в результате исследования Фаном и Чангом \cite{fan_chia_blogger_cca} контекстной рекламы, ими было показано, что их метод может выделить рекламные объявления, которые непосредственно коррелируют с интересами блогера. Для выделения признаков в работе использовался метод хи-квадрат, тогда как для модели классификации использовался метод опорных векторов, изображённый на рисунке \ref{cat};
  \end{itemize}

\item Методы, суть которых заключается в уменьшении размерности данных:
	\begin{itemize}
		\item Латентно-семантический анализ (от англ. Latent Semantic Indexing (LSI))
	\end{itemize}
  
\item Методы, основанные на статистическом подходе:
	\begin{itemize}
		\item Hidden Markov Model (HMM)
		\item Latent Dirichlet Allocation (LDA)
	\end{itemize}

\item Другие методы:
	\begin{itemize}
		\item Метод Мешка слов (от англ. Bag of Words (BoW))
	\end{itemize}

\end{enumerate}

Авторы в \cite{gamal2019analysis} провели исследование по анализу тональности наборов данных, в состав которых входили: Cornell Movie Dataset, Twitter Dataset, Amazon Products Dataset и IMDB Dataset, используя различные методы машинного обучения.
В работе рассматривались следующие методы: Naive Bayes (NB), стохастический градиентный спуск (Stochastic gradient descent - SGD), метод опорных векторов (support vector machine - SVM), пассивный агрессивный метод (Passive Aggressive - PA), максимальной энтропии (Maximum Entropy), Adaptive Boosting (AdaBoost), Multinomial Na?ve Bayes (MNB), Bernoulli Naive Bayes (BNB), Ridge Regression (RR) и Logistic Regression (LR). В результате использования данных методов вместе с методами по извлечению признаков из наборов данных: TF-IDF и N-Грамм (юниграмма, биграмма и триграмма), было получено, что использование алгоритмов PA и RR по сравнению с остальными методами дают одни из самых высоких результатов классификации: от 87\% до 99.96\%, полученный при использовании метода PA. Также было выделено, что метод логистической регрессии и метод опорных векторов также давали приемлемые результаты: 87.56\% и 85.76\% соответственно.


Авторы в \cite{sa_pofonlrev_empty_r} проводят анализ тональности текста (положительный/отрицательный), используя набор данных, состоящий из отзывов, оставленных пользователями на сайте Amazon об электрических товарах (Kindle, DVD и других) в период с февраля 2012 года по июль 2017 года. Для извлечения признаков из данных использовался метод TF-IDF, а для обучения модели были рассмотрены методы машинного обучения: 3 разновидности Naive Bayes классификатора: обычный метод Naive Bayes, Multinomial Naive Bayes и Bernoulli Naive Bayes, а также логистическая регрессия. В итоге были получены следующие результаты: Multinomial Naive Bayes - 92.87\%, Bernoulli Naive Bayes - 92.35\% и Логистическая регрессия - 93.34\%.

Те же самые авторы в \cite{sa_ofofr_cr} изучали применение методов по анализу тональности текста, используя библиотеки языка программирования R по машинному обучению применительно к отзывам покупателей о приобретённой еде на сайте Amazon. В работе описан алгоритм при помощи которого проводилась классификация отзывов, а также приведены облаки слов, которые имели больший вес в модели при определении тональности.

Ещё 1 работа по анализу тональности отзывов с сайта Amazon является работа \cite{bhatt2015amazon}, где авторы предлагают алгоритм по обработке отзывов, призванный разбить оставленные пользователями отзывы на две категории:
	\begin{itemize}
		\item Отзывы о представленном сервисе;
		\item Отзывы о характеристиках продукта;
	\end{itemize}

Далее авторами используется собственный алгоритм, который проводит анализ отзывов на предмет того, является ли отзыв прямым или косвенным. Например: “Камера не лучше, чем у iPhone 4s” - косвенный отзыв, фразы из предложений сравниваются с заранее выделенными в ходе ручной работы с отзывами фразами, по итогу чего и происходит классификация каждого отзыва. Также интересна обработка отрицаний в статье авторов, когда алгоритм встречает в отзыве фразу “не хороший товар”, то сначала алгоритм анализирует тональность слова хороший (стоит также отметить, что он ищет следующую связку: частицу, обозначающую отрицание и прилагательное). После чего, сначала, при помощи словаря тональности определяется тональность прилагательного и, так как ещё есть частица, означающая отрицание, то данной фразе проставляется противоположная тональность.

Алгоритм работы с отзывами, после извлечения набора фраза из предложений, может быть представлен следующим образом:
	\begin{itemize}
		\item Все фразы проверяются на то, являются ли они косвенными или прямыми относительно продукта;
		\item Проверяются на наличие с частицей “не”;
	\end{itemize}




\section{Отчёт об использованных методах}

\subsection{Описание наборов данных}

В данной работе использовался следующий набор данных: отзывы на категории товаров, представленные в разных категориях на сайте Amazon в период с мая 1996 по июль 2014 года.Набор данных был собран в файлы по категориям профессором Стэнфордского университета Джулианом Макаули. \cite{cite_amazon_rewies} 
Были взяты наборы данных по следующим категориям:
\begin{enumerate}
	\item Отзывы на товары категории "Рецензии на книги"
	
	Всего отзывов в категории: 8\,898\,041 записей
	\item Отзывы на товары категории "Электроника"
	
	Всего отзывов в категории: 1\,689\,188
	\item Отзывы на товары категории "Кино и Телевидение"
	
	Всего отзывов в категории: 1\,697\,533
	\item Отзывы на товары категории "Компакт-диски и винилы"
	
	Всего отзывов в категории: 1\,097\,592
	\item Отзывы на товары категории "Kindle Store"
	
	Всего отзывов в категории: 982\,619


\end {enumerate}

Наборы данных были представлены в одинаковом формате JSON\footnote{JavaScript Object Notation - текстовый формат обмена данными, основанный на JavaScript.} и состояли из следующих колонок:

	\begin{itemize}
		\item reviewerID – уникальный идентификатор пользователя
		\item asin – уникальный идентификатор продукта
		\item reviewerName – имя человека, оставившего отзыв
		\item helpful – оценка полезности отзыва
		\item reviewText – текст отзыва
		\item overall – рейтинг, оставленный пользователем (от 1 до 5)
		\item summary – краткий итог отзыва
		\item unixReviewTime/reviewTime – время, когда отзыв был оставлен пользователем
	\end{itemize}
	
Из этих столбцов для нашей задачи будут интересны только reviewText - сам отзыв, оставленный посетителем сайта и overall - оценка пользователя, которая ставится в соответствии с его отзывом.

Итак, обработав отзывы в формате json и переведя их в формат pandas\footnote{Pandas - библиотека языка Python для работы с наборами данных} - DataFrame, мы получили 5 наборов данных. Для начала будем рассматривать в каждом из этих наборов данных максимальное количество записей - 1 млн.

\subsection{Обработка данных}

Первым шагом в данном упражнении, переведём оценки из формата 1-5 в 2 класса, которые понадобятся нам для классификации (-1/1), для этого воспользуемся следующей логикой:

	\begin{itemize}
		\item при overall $\in$ (1,2) присвоим значение -1, что будет означать, что отзывы данной категории имеют отрицательную тональность;
		\item при overall $\in$ (3)  не будем присваивать никакого значения, так как мы будем рассматривать задачу бинарной классификации;
		\item при overall $\in$ (4,5) присвоим значение 1, так как оставляя отзывы с такими оценками, мы можем быть уверены, что пользователь был удовлетворён своей покупкой, а значит отзыв имеет положительную тональность;
	\end{itemize}
	
Имеем 5 наборов данных, состоящих из отзывов пользователей и их оценок, где в каждом наборе данных содержится приблизительно миллион записей. Так как в задачах машинного обучения важно иметь сбалансированные выборки, а в разных категориях товаров разное количество положительных и отрицательных отзывов, то была написана функция, которая считает в каждом наборе данных количество положительных и отрицательных отзывов, берёт наименьшее из них и обрезает класс, где отзывов больше, тем самым получается равное количество положительных и отрицательных отзывов в выборке.
	
\caption{Количество отзывов по классам тональности }

$\begin{tabular}{c|f|f|ff|} 

Название категории & Положительные & Нейтральные & Отрицательные \\ 
\hline 
\hline 
"Рецензии на книги" &  7\,203\,909	&   955\,189	& 738\,943	\\ 
"Кино и Телевидение" & 1\,289\,602	& 201\,302 & 206\,629	\\ 
"Электроника" & 1\,356\,067  & 	142\,257  & 190\,864 	\\ 

"Компакт-диски и винилы" & 903\,002  & 	101\,824  & 92\,766 	\\ 
"Kindle Store" & 829\,277  & 	96\,194  & 57\,148 	\\ 

\hline 
\end{tabular}$


\subsubsection{Векторизация отзывов (метод «Мешка слов»)}
Первым этапом при анализе тех или иных текстов методами машинного обучения, текстовые данные требуется перевести в числовой формат.

Для этого существует определённых набор методик, таких как:
	\begin{itemize}
		\item Метод "Мешок слов" (BoW, bag-of-words) \cite{mcclure2017tensorflow} - метод переводит набор данных в матричный формат, где каждая строка является отдельным предложением или отдельным отзывом, тогда как столбец отвечает за 1 слово в этом предложении/отзыве. Элементами же матрицы являются частоты с которыми данные слова встречаются в предложении.
		\item Метод TF-IDF (TF — term frequency, IDF — inverse document frequency) - метод, суть которого состоит в оценке важности слова как в контексте рассматриваемого экземпляра документа, так и относительно других документов, находящихся в коллекции.
		
		Частота слов (TF) вычисляется как отношение количества вхождений конкретного слова в документе к общему количеству слов в этом документе и записывается в следующем виде:
		
		$tf(a, d) = n_a / \sum_k(n_k)$,
		
		где $n_a$ - количество вхождений слова $a$ в  документ $n$, $\sum_k(n_k)$ - общее число слов в документе $n$.
		
		Обратная частота документа (IDF) - отношение общего количества документов в коллекции $|D|$ к числу документов, где присутствует данное слово $|{d_i \in D | a \in d_i}|$ и записывается в виде:
		
		$idf(a, D) = log (|D|/|{d_i \in D | a \in d_i}|)$
		
		В конечном итоге, меру на которую данный метод опирается, можно записать следующей формулой: $tf-idf(a,d,D) = tf (a, d) * idf (a, D)$

		
		
		\item N-граммы (N-grams) - метод, основанный на методе Мешка слов, однако отличается тем, что данный метод позволяет учесть порядок слов в предложении, что является полезным свойством, особенно при анализе таких предложений как: "Продукт плох, совсем не качественный" или "Продукт качественный, совсем не плохой"\cite{muller2016introduction}. Данные предложения будут иметь одинаковое числовое представление при их обработке методом Мешок слов. Для такого рода ситуаций подходит метод n-граммы, так как позволяет захватить контекст использования данного слова, учитывая не только количество отдельных слов (токенов), но и пар или словосочетаний состоящих из 3-ёх слов. 
	\end{itemize}
	

Удалив из данных знаки препинания и другие лишние символы (отмечу, что они могут быть исследованы при более глубоком анализе) и приведя все слова к нижнему регистру, я последовательно, используя 3 вышеописанных метода, создавал численные представления всех имеющихся отзывов.

\subsubsection{Формальная постановка задачи обучения линейного классификатора}
Формальная постановка рассматриваемой задачи: $X$ - множество объектов, которые представляют из себя текстовые отзывы покупателей о приобретённом товаре. ${x_{1},...,x_{l}}\subset X$ - обучающая выборка. Рассматривается двухклассовая задача, $y_{j} \in (-1,1)$ - признак, который отвечает за тональность отзыва: положительный/отрицательный.
Имеем обучающую выборку: "отзыв - тональность", которая может быть представлена в виде: $X^m = {(x_1, y_1),...,(x_m,y_m)}$
В логистической регрессии строится линейный классификатор $a$ типа \cite{mach_learn_logist_regress}:

$a(x,w) = sign(\sum^n_j_=_1 w_j*f_j(x) - w_0) = sign\langle x,w \rangle$,

где 
\begin{itemize}

\item $w_j$ - вес $j$-го признака
\item $w_0$ - порог принятия решения
\item $w=(w_0, w_1, ..., w_n)$ - вектор весов
\item $\langle x,w \rangle$ - скалярное произведение признаков объекта на вектор весов
\end{itemize}

Задачей является настройка вектора весов $w$ по заданной выборке $X^m$

Логистическая регрессия отличается функцией потерь, которая служит для решения этой задачи и отвечает за минимизацию эмпирического риска, имея вид:

$Q(w) = \sum_i_=_1 {ln(1+exp(-y_i*\langle x_i,w \rangle))} \rightarrow \underset{w}{min}$

После того, как вектор $w$ найден, можно вычислить как классификацию для объекта $x$, так и оценить апостериорные вероятности принадлежности объекта определённому классу:

$P(y|x) = \sigma (y \langle x,w \rangle), y \in Y$, где:
\begin{itemize}
\item $\sigma(z) = 1/(1+e^(-x)$ - сигмоидная функция
\end{itemize}


\subsubsection{Логистическая регрессия}
Для решения данной задачи бинарной классификации, будем использовать логистическую регрессию, которая является одним из методов построения линейного классификатора и даёт оценивать апостериорные вероятности принадлежности объектов классам. 

Так как логистическая регрессия является частным случаем линейного классификатора, то стоит упомянуть, что главная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на два отдельных полупространства. В каждом из полупространств прогнозируется одно (в случае бинарной классификации, из двух) значение целевого класса. Когда это можно сделать без ошибок, то выборка признаётся линейно разделимой.

%Простейший классификатор на основе регрессии выглядит следующим образом: 
%
%$\large a(\vec{x}) = sign(\vec{w}^Tx)$, где:
%\begin{itemize}
%\item \vec{x} - вектор признаков примера  (c учётом единицы, фиктивной )
%\item \vec{w} - 
%\item
%\item
%\end{itemize}


Для решения данной задачи я использовал пакет sklearn, в котором содержится функция LogisticRegression. Мною использовалась штрафная функция типа $L2$, при том, что в пакете существует три варианта регуляризации:
\begin{itemize}
	\item L1 регуляризация:
	
	$\min_{w, c} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1).$
	
	
	\item L2 регуляризация:
	
	$\min_{w, c} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) .$
	
	\item Elastic-Net регуляризация:
	
	$\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)$, где:
	
	\begin{itemize}
		\item коэффициент $p$ отвечает за силу $l_1$ регуляризации и $l_2$ регуляризации.
	\end{itemize}

%Note that, in this notation, it’s assumed that the target  takes values in the set  at trial . We can also see that Elastic-Net is equivalent to  when  and equivalent to  when .

\end{itemize}


——

Также будет добавлено про то, как выбирался (подбирался) показатель регуляризации для классификации.


\subsubsection{Оценка точности классификации}
Исследователями выделяется большое количество оценки точности классификации.

К ним относятся:
\begin{itemize}
	\item ACC ()
	\item Precision
	\item Recall
	\item F1-score
	\item ROC - кривая (кривая ошибок). 
	\item Матрица ошибок (confusion matrix, матрица неточностей) - 
	
\end{itemize}

Самым популярным методом является метод Accuracy: суть заключается в том, что при исследовании точности модели мы смотрим на долю правильных ответов, которая может быть записана в следующем виде:
$Base\text{Rate}=\text{argmax }\frac{1}{l}\sum_{i=1}^{\text{l}}[y_{o}=y_{i}]$
Данную формулу также принято записывать в следующем виде:
$Accuracy\text{Rate}=\frac{\text{TP+TN}}{TP+FP+FN+TN}$

В рассматриваемом примере, данный показатель будет вычисляться на основе сравнения результатов модели с заданными результатами последних 10000 отзывов в каждом наборе данных.

\subsubsection{Результаты}
Результаты работы классификатора:

$\begin{tabular}{c||rrrr|} 
Название категории & AccuracyRate, \% \\ 
\hline 
\hline 
"Рецензии на книги" & 91.47\% \\ 
"Здоровье и личная гигиена" & 85.78\%  \\ 
"Кино и Телевидение" & 90.65\% \\ 
"Электроника" & 89.84\% \\ 
\hline 
\end{tabular}$

Здесь мы можем увидеть результаты работы классификатора, реализованного с использованием логистической регрессии. Интересно изучить слова, которые оказали наибольшее влияние на работу классификатора. Так, например, при анализе отзывов на категорию книги одним из важных факторов для модели при определении тональности стало наличие в предложении автора: Joseph Klausner. Это связано с тем, что под произведениями данного автора, представленными на сайта Amazon, стояло больше всего положительных (как мы помним, оценки 4,5) отзывов, что свидетельствует о любви читателей к определённому автору в рамках рассматриваемого набора данных.




\section{Название третье главы}
%Основная часть
Требуется четко и подробно, шаг за шагом описать, что вы и как сделали. Содержание данного  раздела должно быть таким, чтобы любой человек мог шаг за шагом повторить ваши результаты. 

%Данный раздел будет содержать полученные графики, таблицы и в иной форме результаты ваших экспериментов с подробным описание, как вы их получили. 

\section{Анализ результатов}
%Анализ результатов

После опытов следует анализ полученных результатов (графиков, таблиц и др.), обсуждение полученного, выводы.

\section{Заключение}
Заключение должно содержать основные результаты работы, сделанные вами выводы и выдвинутые гипотезы.

\medskip
\renewcommand{\refname}{Список литературы / References}


{\footnotesize

\begin{thebibliography}{99}

\bibliography{my_booksiki}

\RBibitem{1_int_resource}
\by Сергей Орлов
\paper Журнал о современных тенологиях
\jour КОМПЬЮТЕРРА
\yr 16 января 2019
% \pages 27

\RBibitem{2_int_resource}
\by Rosie Murphy
\paper Local consumer review survey 2018
\jour Веб-сайт
\yr 7 декабря 2018
% \pages 27


\Bibitem {medhat2014sentiment}
\by Medhat, Walaa and Hassan, Ahmed and Korashy, Hoda
\paper Sentiment analysis algorithms and applications: A survey
\jour Ain Shams engineering journal
\yr 2014
\vol 5
\pages 1093--1113


\Bibitem {yu_wu_pmi}
\by Yu Liang-Chih, Wu Jheng-Long, Chang Pei-Chann, Chu Hsuan-Shou
\paper Using a contextual entropy model to expand emotion
words and their intensity for the sentiment classification of stock
market news
\jour Knowl-Based Syst
\yr 2013
\vol 41
\pages 89–97


\Bibitem {fan_chia_blogger_cca}
\by Teng-Kai Fan, Chia-Hui Chang
\paper Blogger-Centric Contextual Advertising
\jour Expert Systems with Applications
\yr 2011
\vol 38
\issue 3
\pages 1777-1788




\Bibitem {gamal2019analysis}
\by Gamal, Donia and Alfonse, Marco and M El-Horbaty, El-Sayed and M Salem, Abdel-Badeeh
\paper Analysis of Machine Learning Algorithms for Opinion Mining in Different Domains
\jour Machine Learning and Knowledge Extraction
\publ Multidisciplinary Digital Publishing Institute
\yr 2019
\vol 1
\pages 224--234

\Bibitem {sa_pofonlrev_empty_r}
\by Sasikala P, L.Mary Immaculate Sheela
\paper Sentiment Analysis and Prediction of Online Reviews with Empty Ratings
\jour International Journal of Applied Engineering Research
\publ Research India Publications
\yr 2018
\vol 13
\pages 11525--11531


\Bibitem {sa_ofofr_cr}
\by Sasikala P, L.Mary Immaculate Sheela
\paper Sentiment Analysis of Online Food Reviews using
Customer Ratings
\jour International Journal of Pure and Applied Mathematics
\publ 
\yr 2018
\vol 119
\pages 3509--3514

\Bibitem {bhatt2015amazon}
\by Bhatt, Aashutosh and Patel, Ankit and Chheda, Harsh and Gawande, Kiran
\paper Amazon Review Classification and Sentiment Analysis
\jour International Journal of Computer Science and Information Technologies
\publ Citeseer
\yr 2015
\vol 6
\pages 5107--5110

\Bibitem {mcclure2017tensorflow}
\by McClure, Nick
\paper TensorFlow machine learning cookbook
\publ Packt Publishing Ltd
\yr 2017
\pages 185--187


\Bibitem {muller2016introduction}
\by M{\"u}ller, Andreas C and Guido, Sarah and others
\paper Introduction to machine learning with Python: a guide for data scientists
\publ  O'Reilly Media, Inc.
\yr 2016
\pages 339--341


\Bibitem {cite_amazon_rewies}
\by узнать как сайты оформлять




\Bibitem {mach_learn_logist_regress}
\by узнать как сайты оформлять
http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F


%\RBibitem{1}
%\by Мальцев~А.\,И.
%\paper Об изоморфном представлении бесконечных групп матрицами
%\jour Мат. сб.
%\yr 1940
%\vol 8
%\issue 3
%\pages 405--422
%\transl
%[\by Malcev~A.\,I.
%\paper Ob izomorfnom predstavlenii beskonechnykh grupp matritsami
%\jour Mat. sb.
%\yr 1940
%\vol 8
%\issue 3
%\pages 405--422
%\finalinfo (in Russian).\nofrills]
%
%\Bibitem{2}
%\by Hirsh~K.\,A.
%\paper On infinite soluble groups
%\jour J. London Math. Soc.
%\yr 1952
%\vol 27
%\pages 81--85
%
%\Bibitem{L}
%\by Learner~A.
%\paper Residual properties of polycyclic groups
%\jour J. Math.
%\yr 1964
%\vol 8
%\pages 536--542
%
%\RBibitem{Se}
%\by Сексенбаев~К.
%\paper К теории полициклических групп
%\jour Алгебра и логика
%\yr 1965
%\vol 4
%\issue 3
%\pages 79--83
%\rtransl
%[\by Seksenbaev~K.
%\paper K teorii policiklicheskih grupp
%\jour Algebra i logika
%\yr 1965
%\vol 4
%\issue 3
%\pages 79--83
%\finalinfo (in Russian).\nofrills]
%
%\RBibitem{3}
%\by Шмелькин~А.\,И.
%\paper Полициклические группы
%\jour Сиб. мат. ж.
%\yr 1968
%\vol  9
%\pages 234--235
%\transl
%[\by Smelkin~A.\,L.
%\paper Politsiklicheskie gruppy
%\jour Sib. mat. zh.
%\yr 1968
%\vol  9
%\pages 234--235
%\finalinfo (in Russian).\nofrills]
%
%\Bibitem{4}
%\by Gruenberg~K.\,W.
%\paper Residual properties of infinite soluble groups
%\jour Proc. London Math. Soc.
%\yr 1957
%\vol 3
%\issue 7
%\issue 25
%\pages 29--62
%
%
%\bibitem{} 



\end{thebibliography}
}
\vskip 8pt
   \smallskip\hrule width 6cm
\medskip

\end{document}





%Backup
%Тогда векторное описание объекта имеет следующий вид:

%$F=||f_{j}(x_{i})||_{l*n}= \left(\begin{array}{cccc} f_{1}(x_1) & \ldots & f_{n}(x_1)\\ \vdots & \ddots & \vdots\\ f_{1}(x_l) & \ldots & f_{n}(x_l) \end{array} \right)$


