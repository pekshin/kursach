{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:15:03.238307Z",
     "start_time": "2019-05-28T10:15:03.215641Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "######## METRICS\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "# matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:15:03.616972Z",
     "start_time": "2019-05-28T10:15:03.609989Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize   \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:15:04.387095Z",
     "start_time": "2019-05-28T10:15:04.378127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print(porter.stem(\"loved\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:15:04.878197Z",
     "start_time": "2019-05-28T10:15:04.870367Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:15:05.347800Z",
     "start_time": "2019-05-28T10:15:05.335028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love dog rocket work essenti thi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(str_):\n",
    "    lst = str_.split(\" \")\n",
    "    lst_new = []\n",
    "    for i in lst:\n",
    "        new = porter.stem(i)\n",
    "        lst_new.append (new)\n",
    "    return ' '.join(lst_new)\n",
    "df = ['loved dogs rockets works essential this','rockets fucking']\n",
    "\n",
    "lemmatize (df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:15:06.392049Z",
     "start_time": "2019-05-28T10:15:06.338602Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_it(filename, parametr): \n",
    "    i = 0\n",
    "    reviews_lst_good= []\n",
    "    reviews_lst_bad = []\n",
    "\n",
    "    #набираем список хороших/плохих отзывов, не более 500000 в каждом из списков\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['overall'] > 3 and len(reviews_lst_good)<=500000:\n",
    "                    reviews_lst_good.append (json.loads(line))\n",
    "\n",
    "                elif json.loads(line)['overall'] < 3 and len(reviews_lst_bad)<=500000:\n",
    "                    reviews_lst_bad.append (json.loads(line))\n",
    "\n",
    "                if len(reviews_lst_good) + len (reviews_lst_bad) == 1000000:\n",
    "                    break\n",
    "    # random elements in the list\n",
    "    \n",
    "    # перемешиваем список\n",
    "    \n",
    "    random.shuffle(reviews_lst_good)\n",
    "    random.shuffle(reviews_lst_bad)  \n",
    "    \n",
    "    # создаём template для сохранения конечных отзывов\n",
    "    reviewerID_lst = []\n",
    "    reviewerName_lst = []\n",
    "    reviewText_lst = []\n",
    "    overall_lst = []\n",
    "    summary_lst = []\n",
    "    \n",
    "    # где будем уменьшать количество отзывов, дабы их было равное количество\n",
    "    \n",
    "    if len (reviews_lst_bad) < len(reviews_lst_good):\n",
    "        count_otz = len (reviews_lst_bad)\n",
    "    else:\n",
    "        count_otz = len (reviews_lst_good)\n",
    "\n",
    "    # обрезаем хороший\n",
    "    for i in reviews_lst_good[:count_otz]:\n",
    "        reviewText_lst.append (i['reviewText'])\n",
    "        overall_lst.append (i['overall'])\n",
    "\n",
    "    print ('Good: ',len(reviews_lst_good[:count_otz]))\n",
    "\n",
    "    # обрезаем плохие\n",
    "    for i in reviews_lst_bad[:count_otz]:\n",
    "        reviewText_lst.append ((i['reviewText']))\n",
    "        overall_lst.append (i['overall'])\n",
    "        \n",
    "    print ('Bad: ',len(reviews_lst_bad[:count_otz]))\n",
    "    \n",
    "#     test_len = int(round (len(reviews_lst_bad[:count_otz])*0.1,0))\n",
    "#     #длина выборки, на которой строим тесты  \n",
    "#     print ('10%: ',test_len)\n",
    "\n",
    "    df = pd.DataFrame ()\n",
    "    \n",
    "    reviewText_lst_n = []\n",
    "    \n",
    "    \n",
    "    # lemmatize\n",
    "    for i in reviewText_lst:\n",
    "        reviewText_lst_n.append (lemmatize (i))\n",
    "    \n",
    "    df['reviewText'] = reviewText_lst_n\n",
    "    df['overall'] = overall_lst\n",
    "    \n",
    "    \n",
    "    # 0 и 1 ставим\n",
    "    df ['mark'] = 0\n",
    "    df.loc[df['overall'] > 3,'mark'] = 1\n",
    "    \n",
    "    # количество отзывов в нормальной и тестовой выборке\n",
    "    reviews_train = list(df[df['overall'] > 3]['reviewText']) + list(df[df['overall'] < 3]['reviewText'])\n",
    "\n",
    "    # очищаем отзывы от всяких лишних символов\n",
    "    reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "    \n",
    "    # бинаризируем столбец признаков\n",
    "    cv = TfidfVectorizer(binary=True)\n",
    "    cv.fit(reviews_train_clean)\n",
    "    \n",
    "    # к норм виду приводим\n",
    "    X = cv.transform(reviews_train_clean)\n",
    "\n",
    "    \n",
    "    # столбец ответов \n",
    "    y = list(df[df['overall'] > 3]['mark']) + list(df[df['overall'] < 3]['mark'])\n",
    "    \n",
    "    \n",
    "    # делим выборку на тестовую и обучающую\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, train_size = 0.75\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print ('Подбор параметра для: ', filename[:-5])\n",
    "    print ('    ')\n",
    "    \n",
    "    \n",
    "    best_c_dict = {}\n",
    "    main_dict = {}\n",
    "    c_list = []\n",
    "    result_1_list = []\n",
    "    result_2_list = []\n",
    "    \n",
    "    # определяем, где алгоритму необходимо искать С\n",
    "    grid = np.power(10.0, np.arange(-4, 4))\n",
    "    \n",
    "    # итерируемся и подбираем параметр, используя Accuracy\n",
    "    for c in grid:\n",
    "        lr = LogisticRegression(C=c, solver= 'sag')\n",
    "        lr.fit(X_train, y_train)\n",
    "        print (\"Accuracy for C=%s: %s\" \n",
    "               % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "        \n",
    "        c_list.append (c)\n",
    "        result_1_list.append (accuracy_score(y_val, lr.predict(X_val)))\n",
    "\n",
    "        best_c_dict [c] = accuracy_score(y_val, lr.predict(X_val))\n",
    "        main_dict [c] = [filename, accuracy_score(y_val, lr.predict(X_val))]\n",
    "        \n",
    "    \n",
    "    # записываем результаты\n",
    "    result = pd.DataFrame()\n",
    "    result['c'] = c_list\n",
    "    result ['result_1'] = result_1_list\n",
    "    result.to_csv (filename[:-5]+'_podbor_c_c1.csv', index=False)\n",
    "    ###############\n",
    "    \n",
    "    inverse = [(value, key) for key, value in best_c_dict.items()]\n",
    "    print ('The best C is ', max(inverse)[1])\n",
    "\n",
    "    # находим лучший результат и записываем его + используем для модели\n",
    "    \n",
    "    final_model = LogisticRegression(C=max(inverse)[1])\n",
    "    print ('So model use C = ', max(inverse)[1])\n",
    "    final_model.fit(X_train,  y_train)\n",
    "    print (\"Final Accuracy: %s\" \n",
    "           % accuracy_score(y_val\n",
    "                            , final_model.predict(X_val)))\n",
    "    \n",
    "    print('confusion_matrix')\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_val, final_model.predict(X_val))\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    print ('classification_report')\n",
    "    print(classification_report(y_val, final_model.predict(X_val)))\n",
    "    \n",
    "    \n",
    "    print ('roc crivai')\n",
    "    \n",
    "\n",
    "    logit_roc_auc = roc_auc_score(y_val, final_model.predict(X_val))\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, final_model.predict_proba(X_val)[:,1])\n",
    "    plt.figure()\n",
    "    \n",
    "    \n",
    "    data = {'fpr': fpr, 'tpr': tpr}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv (filename[:-5]+'_roc_crivai_v1.csv', index=False)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    \n",
    "    # выделяем слова, которые обладают самым большим весом\n",
    "    feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "        )\n",
    "    }\n",
    "    for best_positive in sorted(\n",
    "        feature_to_coef.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True)[:25]:\n",
    "        print (best_positive)\n",
    "    \n",
    "\n",
    "    word_lst = []\n",
    "    score_lst = []\n",
    "    for i in best_positive:\n",
    "        word_lst.append (i)\n",
    "        score_lst.append (best_positive[i])\n",
    "    \n",
    "    data = {'word': word_ls, 'level': score_lst}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv (filename[:-5]+'_posit_words_v1.csv', index=False)\n",
    "    \n",
    "    for best_negative in sorted(\n",
    "        feature_to_coef.items(), \n",
    "        key=lambda x: x[1])[:25]:\n",
    "        print (best_negative)\n",
    "    \n",
    "    word_lst = []\n",
    "    score_lst = []\n",
    "    for i in best_negative:\n",
    "        word_lst.append (i)\n",
    "        score_lst.append (best_negative[i])\n",
    "    \n",
    "    data = {'word': word_ls, 'level': score_lst}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv (filename[:-5]+'_negat_words_v1.csv', index=False)\n",
    "        \n",
    "    del reviews_lst_good\n",
    "    del reviews_lst_bad\n",
    "    \n",
    "    return main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-28T10:15:07.256Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health_and_Personal_Care_5\n",
      "   \n",
      "Good:  33300\n",
      "Bad:  33300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подбор параметра для:  /Volumes/GoogleDrive/Мой диск/Work/Experiment/Health_and_Personal_Care_5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.listdir('/Volumes/GoogleDrive/Мой диск/Work/Experiment')\n",
    "\n",
    "for i in os.listdir('/Volumes/GoogleDrive/Мой диск/Work/Experiment'):\n",
    "    if i[-5:] != 'ipynb'and i[-3:] != 'csv' and i != '.ipynb_checkpoints': # and i = 'Health_and_Personal_Care_5.json':\n",
    "        print (i[:-5])\n",
    "        print ('   ')\n",
    "        dict_main = classify_it ('/Volumes/GoogleDrive/Мой диск/Work/Experiment/' + i, 0.25)\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T06:14:29.423202Z",
     "start_time": "2019-05-21T06:13:30.942Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.mlab as mlab\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# x_lst = []\n",
    "# y_lst = []\n",
    "# for i in dict_main:\n",
    "#     x_lst.append(i)\n",
    "#     y_lst.append(dict_main[i][1])\n",
    "\n",
    "# plt.bar(x_lst, y_lst, align='center', alpha=0.3, width=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T06:14:29.426304Z",
     "start_time": "2019-05-21T06:13:30.949Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame( = [1,2,3], b = [4,5,6])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "\n",
    "https://habr.com/ru/company/ods/blog/323890/#1-lineynaya-regressiya\n",
    "\n",
    "https://docplayer.ru/28834116-Otchet-po-zadaniyu-4-issledovanie-modeli-logistic-regression.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_(%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9,_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2)#.D0.9E.D1.81.D0.BD.D0.BE.D0.B2.D0.BD.D1.8B.D0.B5_.D0.BF.D0.BE.D0.BD.D1.8F.D1.82.D0.B8.D1.8F_.D0.B8_.D0.BF.D1.80.D0.B8.D0.BC.D0.B5.D1.80.D1.8B_.D0.BF.D1.80.D0.B8.D0.BA.D0.BB.D0.B0.D0.B4.D0.BD.D1.8B.D1.85_.D0.B7.D0.B0.D0.B4.D0.B0.D1.87\n",
    "\n",
    "\n",
    "\n",
    "ROC:\n",
    "https://basegroup.ru/community/articles/logistic\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
