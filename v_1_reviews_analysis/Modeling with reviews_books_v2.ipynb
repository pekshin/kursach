{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T10:54:24.386466Z",
     "start_time": "2019-04-08T10:54:22.422283Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gengo.ai/datasets/15-free-sentiment-analysis-datasets-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T10:56:14.576854Z",
     "start_time": "2019-04-08T10:54:27.090909Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i = 0\n",
    "reviews_lst_good= []\n",
    "reviews_lst_bad = []\n",
    "\n",
    "\n",
    "with open('C:\\\\Users\\\\denis.pekhterev\\\\Downloads\\\\Experiment_old\\\\Books_5.json', 'r') as f:\n",
    "        for line in f:\n",
    "            if json.loads(line)['overall'] > 3 and len(reviews_lst_good)<=500000:\n",
    "                reviews_lst_good.append (json.loads(line))\n",
    "            \n",
    "            elif json.loads(line)['overall'] < 3 and len(reviews_lst_bad)<=500000:\n",
    "                reviews_lst_bad.append (json.loads(line))\n",
    "\n",
    "            if len(reviews_lst_good) + len (reviews_lst_bad) == 1000000:\n",
    "                break\n",
    "                \n",
    "random.shuffle(reviews_lst_good)\n",
    "random.shuffle(reviews_lst_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T10:56:54.031230Z",
     "start_time": "2019-04-08T10:56:51.398664Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewerID_lst = []\n",
    "reviewerName_lst = []\n",
    "reviewText_lst = []\n",
    "overall_lst = []\n",
    "summary_lst = []\n",
    "\n",
    "# reviews_lst [0]['overall']<5 #['asin']for \n",
    "\n",
    "for i in reviews_lst_good:\n",
    "    reviewerID_lst.append (i['reviewerID'])\n",
    "    reviewText_lst.append (i['reviewText'])\n",
    "    overall_lst.append (i['overall'])\n",
    "    summary_lst.append (i['summary'])\n",
    "    \n",
    "for i in reviews_lst_bad:\n",
    "    reviewerID_lst.append (i['reviewerID'])\n",
    "    reviewText_lst.append (i['reviewText'])\n",
    "    overall_lst.append (i['overall'])\n",
    "    summary_lst.append (i['summary'])\n",
    "    \n",
    "df = pd.DataFrame ()\n",
    "\n",
    "df['reviewerID'] = reviewerID_lst\n",
    "# df['reviewerName'] = reviewerName_lst\n",
    "df['reviewText'] = reviewText_lst\n",
    "df['overall'] = overall_lst\n",
    "df['summary'] = summary_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T10:57:44.606899Z",
     "start_time": "2019-04-08T10:57:43.726984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500001\n"
     ]
    }
   ],
   "source": [
    "df ['mark'] = 0\n",
    "df.loc[df['overall'] > 3,'mark'] = 1\n",
    "\n",
    "print (len(df[df['mark'] > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T10:58:36.932884Z",
     "start_time": "2019-04-08T10:58:36.800817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [reviewerID, reviewText, overall, summary, mark]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['mark'] != 0)&(df['mark'] != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T10:59:17.142341Z",
     "start_time": "2019-04-08T10:59:16.481615Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews_train = list(df[df['overall'] > 3]['reviewText'][:-10000]) + list(df[df['overall'] < 3]['reviewText'][:-10000])\n",
    "reviews_test = list(df[df['overall'] > 3]['reviewText'][-10000:]) + list(df[df['overall'] < 3]['reviewText'][-10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:00:14.838127Z",
     "start_time": "2019-04-08T11:00:14.829696Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Insurgent, book 2 in the Divergent series is another of my most anticipated reads for this year and it held me captivated from the very first page.I honestly don't know where to start! Divergent for me was all sorts of amazing; it was everything I love in a dystopian novel and although I really enjoyed Insurgent, the more morose feel seemed to bring it down a level for me.Insurgent takes us through the aftermath of Divergent, lives have been lost, loyalties have been divided and the factions are more or less at war. Tris Prior has a lot of guilt on her shoulders, she is suffering terribly with what she has been through and what she has lost; her strength and determination were still present but along with it came the negativity and her grief, she was dealing with a lot of emotion and it certainly came out through her sometimes reckless and selfish actions.Four/Tobias makes a few revelations of his own, I absolutely adore him as a character; he is strong, protective and determined and I enjoy the way he holds Tris together when she needs it most. Although their relationship had a few ups and downs, I still thought they complimented each other well.There is a lot of faction politics in this book and we get to learn a lot more about the Dauntless, Abnegation, Amity, Candor and the Erudite; but to add to the mix, we also get to know more about the factionless and I was taken completely by surprised by who their leader was.I wish at the beginning of this book there was more mention of the events which had taken place in Divergent; having read this book so long ago I should have given myself a refresher but at times I felt a little lost with what was going on.Each of the secondary characters are fleshed out well and play an important role in this book, I was shocked and surprised by a few and disappointed in others but all in all each were enjoyable and suited their own factions perfectly.For the most part, the pacing flowed well; the action was ongoing and there was always some drama or another to keep me entertained and engrossed. I thoroughly enjoy Ms Roth's powerful writing; she hooks me in from the start and doesn't let up. The world building is truly amazing!I was on edge throughout most of this book with the betrayals, danger, conspiracies, deceit and conflict but within it all a sweet yet at times complicated romance which made for a thoroughly enjoyable read. I am so shocked by the cliffhanger ending so I am eager to get hold of the next installment which is due to be released in 2013 (ARGH).\",\n",
       " 'In hardly more than 200 pages, Ernest Becker has written a book of remarkable and startling insight into human nature, the culture that arises from that nature, and how to see what most people cannot: the way we\\'ve been programmed from infancy to cope with the anxieties of the human condition according to the symbol systems imparted to us by our parents and our society.Culture, religion, political ideals...they are nothing but neurotic defenses against existential terror.We\\'re born from out of nowhere and dissolved back into nowhere--and the anxiety this produces we must somehow forget if we\\'re to get on with the day-to-day business of living. But as a result, we end up living largely shallow lives, finding \"meaning\" in material pleasures and possessions, in patriotism, professions, catechisms of one sort or another, even in parenthood because that\\'s what our society rewards us for doing.But do these pursuits really satisfy--or are they only neurotic responses to feelings of powerlessness and fears of meaninglessness in the face of an inescapable death we\\'d rather do anything than face?Becker lucidly traces our development as individuals and as a species from a basic sense of helplessness to a mastery of our environment through the manipulation of symbols, primarily language, self-reflection, and abstract thinking. This mastery is, in fact, a desperate and necessary quest for self-esteem in the face of our cosmic irrelevance that is literally a matter of life and death.That seemingly stupid and pointless exchange of nods and raised eyebrows that transpires when you pass a workmate in the office--it\\'s loaded with codes and cues. The dumb small-talk you\\'re compelled to make at cocktail parties--it fulfills a social contract whose terms we\\'ve agreed to by default, just by being a \"human\" being. We are all engaging in a drama, each with our parts to play, and if you don\\'t play yours, the rest will turn against you because what you are doing is threatening to expose the whole show as nothing but a charade. The unemployed, the ostracized, the homeless, the lonely, those committed to prisons and mental hospitals--their ranks are filled with people who, for one reason or another, cannot play along successfully.Most people can\\'t handle the truth--which is largely how the world keeps going round.Becker is talking to those who can. He urges those strong enough to cast off the fictions we live by, the fetters that bind us, the falsehoods that protect us from fear--but that also keep us from authentic living. Because even if we play along, many of us are unhappy, even if we don\\'t realize why. The world is a violent place filled with neurotic and psychopathic \"normal\" people...society itself is a neurotic response taken by the majority to an intolerable condition. Instead of merely playing our roles, Becker calls us to a new kind of religious sensibility--one that asks questions rather than repeat traditional answers. A religious sensibility--not a religion--that enables us to hold in balance our paradoxical position somewhere between god and animal.The goal, Becker seems to say, is to choose a role for ourselves but never forget that it is a role. Like the existentialists, Becker suggests that the \"meaning\" of life is the meaning we give it--but that\\'s \"all\" it is, the meaning we\\'ve decided to give it. And to be truly free is to never become so wholly lost in the role we\\'ve assigned ourselves, nor the drama we\\'ve written to star in that we mistake ourselves for our role or the drama for reality.We are, in fact, what lies behind all that--an actor whose face we never see in full light, who appears on stage and disappears off-stage, who remains unknown even when the final credits roll.It may well be that most people cannot endure such uncertainty--nor so much freedom. And, sadly, that\\'s why the world is in the sorry condition it\\'s in, has always been, and most likely will always be.But \"I\" is a candle that can only be lit one at a time. That\\'s the good news Becker delivers in this bluntly provocative but ultimately inspiring book. If you\\'ve often felt like a character in a Twilight Zone episode, the one sane person in a lunatic asylum, Becker is good company. You\\'re almost certain to enjoy his work.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(df[df['overall'] > 3]['reviewText'][:2])\n",
    "reviews_train [:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:02:23.445901Z",
     "start_time": "2019-04-08T11:01:12.812603Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:09:28.284281Z",
     "start_time": "2019-04-08T11:04:27.780704Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:12:00.928316Z",
     "start_time": "2019-04-08T11:11:50.789623Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = list(df[df['overall'] > 3]['mark'][:-10000]) + list(df[df['overall'] < 3]['mark'][:-10000])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "# for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "#     lr = LogisticRegression(C=c)\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     print (\"Accuracy for C=%s: %s\" \n",
    "#            % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:31:27.359924Z",
     "start_time": "2019-04-08T11:16:52.330314Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.91515\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=0.5)\n",
    "final_model.fit(X,  y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score( list(df[df['overall'] > 3]['mark'][-10000:]) + list(df[df['overall'] < 3]['mark'][-10000:])\n",
    "                        , final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:34:31.881045Z",
     "start_time": "2019-04-08T11:34:23.947883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('klausner', 4.200585683240843)\n",
      "('ariely', 2.8356831815044803)\n",
      "('everneath', 2.6124595643870463)\n",
      "('maxon', 2.548042693956325)\n",
      "('spong', 2.503794565750101)\n",
      "('booksneeze', -3.4660027102033775)\n",
      "('waterbrook', -2.7733974365937835)\n",
      "('killgore', -2.7173856084710564)\n",
      "('scarpetta', -2.6039862116577344)\n",
      "('booksneezecom', -2.5929997283216117)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "    )\n",
    "}\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print (best_positive)\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:37:59.754693Z",
     "start_time": "2019-04-08T11:37:56.101655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('klausner', 4.200585683240843)\n",
      "('ariely', 2.8356831815044803)\n",
      "('everneath', 2.6124595643870463)\n",
      "('maxon', 2.548042693956325)\n",
      "('spong', 2.503794565750101)\n",
      "('olaf', 2.335943150736356)\n",
      "('ehrman', 2.2755094891117817)\n",
      "('pleasantly', 2.2397345696720072)\n",
      "('aria', 2.227326464942671)\n",
      "('margolin', 2.209052535420335)\n",
      "('erdrich', 2.016718935236432)\n",
      "('wynne', 2.001600642188314)\n",
      "('proses', 1.9956662863936512)\n",
      "('tamani', 1.9886643508909623)\n",
      "('kaiden', 1.9801171303234608)\n",
      "('hobb', 1.9683015005540692)\n",
      "('maclean', 1.952409460033418)\n",
      "('zinsser', 1.94943085254741)\n",
      "('lobdell', 1.9294905306995673)\n",
      "('argeneau', 1.925562957231128)\n",
      "('halpern', 1.915425482499514)\n",
      "('wallis', 1.9136862005436155)\n",
      "('rollins', 1.9082115838869715)\n",
      "('humbert', 1.9015820908296284)\n",
      "('refreshing', 1.9004044314727193)\n",
      "('gaiman', 1.8980815479534694)\n",
      "('quibble', 1.8979643410512592)\n",
      "('harpercollins', 1.8927034055101044)\n",
      "('alafair', 1.8753738127073951)\n",
      "('dany', 1.853341642609548)\n",
      "('gitomer', 1.8495983741606035)\n",
      "('septimus', 1.8426698293836294)\n",
      "('refreshingly', 1.8256662774658516)\n",
      "('renni', 1.8256196890411347)\n",
      "('quibbles', 1.8232014639190295)\n",
      "('postsecret', 1.8142724698162946)\n",
      "('toole', 1.8101828847793728)\n",
      "('coetzee', 1.8059968158101514)\n",
      "('bravo', 1.8022502455206304)\n",
      "('baudelaires', 1.7924734060576915)\n",
      "('crownover', 1.7792581644463012)\n",
      "('kerrelyn', 1.7752349962199256)\n",
      "('lippman', 1.7702892073896135)\n",
      "('bukowski', 1.7702828808908073)\n",
      "('biscuit', 1.7642307291116026)\n",
      "('lashner', 1.7613649998571577)\n",
      "('chinaski', 1.7582105243697648)\n",
      "('boortz', 1.757610453737932)\n",
      "('sobering', 1.738749849723417)\n",
      "('wiesels', 1.738213984430764)\n",
      "('fayz', 1.7236276224091018)\n",
      "('borgs', 1.720844839667965)\n",
      "('unearthly', 1.7198874849355852)\n",
      "('schaums', 1.7194643171819914)\n",
      "('sandman', 1.7080543125069518)\n",
      "('lurie', 1.7005045356364317)\n",
      "('crombie', 1.7004168826607273)\n",
      "('sabriel', 1.6944459183986786)\n",
      "('punches', 1.6943694122823385)\n",
      "('getabstract', 1.6854660634062675)\n",
      "('langoliers', 1.6807966851407454)\n",
      "('enzo', 1.6794405037114941)\n",
      "('meena', 1.677953036723321)\n",
      "('hoffer', 1.67035186614164)\n",
      "('eloisa', 1.6646399625268233)\n",
      "('vail', 1.6577007812816127)\n",
      "('weggins', 1.6556926071337525)\n",
      "('grippando', 1.6528688103173699)\n",
      "('jacinda', 1.6528408140845707)\n",
      "('stossels', 1.6495004084030132)\n",
      "('murakami', 1.6471635333388226)\n",
      "('lorien', 1.6469885261042518)\n",
      "('patchett', 1.6463721422084163)\n",
      "('disappoint', 1.6459043703512062)\n",
      "('caylee', 1.6395650970823101)\n",
      "('ranney', 1.633437034608241)\n",
      "('bourdain', 1.6310668157043604)\n",
      "('hurston', 1.6292060154448584)\n",
      "('angstrom', 1.6288251961099423)\n",
      "('apprehensive', 1.6282111464787041)\n",
      "('jenkins', 1.6210970802007953)\n",
      "('mckee', 1.6204577354256626)\n",
      "('shepard', 1.620051258555359)\n",
      "('lyssa', 1.619845199675981)\n",
      "('ree', 1.6169237285869014)\n",
      "('stegner', 1.5961163892147088)\n",
      "('maupin', 1.5935904009303177)\n",
      "('monogamy', 1.586902094174137)\n",
      "('quotdeep', 1.5854281869751947)\n",
      "('janelle', 1.5834596445010016)\n",
      "('steinbeck', 1.571791191521653)\n",
      "('pitera', 1.5702185767912296)\n",
      "('sugarcreek', 1.5656044289003255)\n",
      "('karr', 1.5648912393736727)\n",
      "('carmack', 1.5621101760713454)\n",
      "('naysayers', 1.5590933233160071)\n",
      "('fuhrman', 1.5572814812130658)\n",
      "('carraway', 1.556891815430641)\n",
      "('schlosser', 1.5556329139143952)\n",
      "('jeaniene', 1.5550846897522643)\n",
      "('discworld', 1.5510588188390733)\n",
      "('guhrke', 1.5501867770639557)\n",
      "('shriver', 1.547848792980507)\n",
      "('jinni', 1.5426831171476483)\n",
      "('vermeers', 1.5366555100902748)\n",
      "('kingsolver', 1.5348464181092278)\n",
      "('spongs', 1.5322801786132378)\n",
      "('annabelle', 1.5309811066840067)\n",
      "('sigma', 1.53042571149134)\n",
      "('complaint', 1.530387684673951)\n",
      "('langella', 1.5292112840563008)\n",
      "('allon', 1.52825271649736)\n",
      "('roarks', 1.5260318441123084)\n",
      "('addicting', 1.5231794530787437)\n",
      "('soma', 1.5228988787802888)\n",
      "('gatsbys', 1.5221156606088444)\n",
      "('hodel', 1.5215166477623883)\n",
      "('flashman', 1.521340618501774)\n",
      "('delightfully', 1.5194045754346468)\n",
      "('gutman', 1.5189480435083114)\n"
     ]
    }
   ],
   "source": [
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:120]:\n",
    "    print (best_positive)\n",
    "\n",
    "writers = ['klausner','rollins','ehrman','gaiman','spong','olaf','steinbeck','harpercollins','discworld',\n",
    "'kingsolver','pratchett','maxon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T12:07:17.875433Z",
     "start_time": "2019-04-08T12:07:17.822327Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def classify_it(filename, parametr): \n",
    "    i = 0\n",
    "    reviews_lst_good= []\n",
    "    reviews_lst_bad = []\n",
    "\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['overall'] > 3 and len(reviews_lst_good)<=500000:\n",
    "                    reviews_lst_good.append (json.loads(line))\n",
    "\n",
    "                elif json.loads(line)['overall'] < 3 and len(reviews_lst_bad)<=500000:\n",
    "                    reviews_lst_bad.append (json.loads(line))\n",
    "\n",
    "                if len(reviews_lst_good) + len (reviews_lst_bad) == 1000000:\n",
    "                    break\n",
    "    \n",
    "    random.shuffle(reviews_lst_good)\n",
    "    random.shuffle(reviews_lst_bad)\n",
    "                    \n",
    "    reviewerID_lst = []\n",
    "    reviewerName_lst = []\n",
    "    reviewText_lst = []\n",
    "    overall_lst = []\n",
    "    summary_lst = []\n",
    "\n",
    "    # reviews_lst [0]['overall']<5 #['asin']for \n",
    "\n",
    "    for i in reviews_lst_good:\n",
    "        reviewerID_lst.append (i['reviewerID'])\n",
    "        reviewText_lst.append (i['reviewText'])\n",
    "        overall_lst.append (i['overall'])\n",
    "        summary_lst.append (i['summary'])\n",
    "\n",
    "    for i in reviews_lst_bad:\n",
    "        reviewerID_lst.append (i['reviewerID'])\n",
    "        reviewText_lst.append (i['reviewText'])\n",
    "        overall_lst.append (i['overall'])\n",
    "        summary_lst.append (i['summary'])\n",
    "\n",
    "    df = pd.DataFrame ()\n",
    "\n",
    "    df['reviewerID'] = reviewerID_lst\n",
    "    # df['reviewerName'] = reviewerName_lst\n",
    "    df['reviewText'] = reviewText_lst\n",
    "    df['overall'] = overall_lst\n",
    "    df['summary'] = summary_lst\n",
    "    \n",
    "    df ['mark'] = 0\n",
    "    df.loc[df['overall'] > 3,'mark'] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    reviews_train = list(df[df['overall'] > 3]['reviewText'][:-10000]) + list(df[df['overall'] < 3]['reviewText'][:-10000])\n",
    "    reviews_test = list(df[df['overall'] > 3]['reviewText'][-10000:]) + list(df[df['overall'] < 3]['reviewText'][-10000:])\n",
    "\n",
    "\n",
    "\n",
    "    reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "    reviews_test_clean = preprocess_reviews(reviews_test)\n",
    "    \n",
    "    cv = CountVectorizer(binary=True)\n",
    "    cv.fit(reviews_train_clean)\n",
    "    X = cv.transform(reviews_train_clean)\n",
    "    X_test = cv.transform(reviews_test_clean)\n",
    "    \n",
    "    y = list(df[df['overall'] > 3]['mark'][:-10000]) + list(df[df['overall'] < 3]['mark'][:-10000])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, train_size = 0.75\n",
    "    )\n",
    "    \n",
    "    print ('Подбор параметра для: ', filename)\n",
    "    \n",
    "    for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "        lr = LogisticRegression(C=c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        print (\"Accuracy for C=%s: %s\" \n",
    "               % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "    \n",
    "    final_model = LogisticRegression(C=parametr)\n",
    "    final_model.fit(X,  y)\n",
    "    print (\"Final Accuracy: %s\" \n",
    "           % accuracy_score( list(df[df['overall'] > 3]['mark'][-10000:]) + list(df[df['overall'] < 3]['mark'][-10000:])\n",
    "                            , final_model.predict(X_test)))\n",
    "    \n",
    "    feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "        )\n",
    "    }\n",
    "    for best_positive in sorted(\n",
    "        feature_to_coef.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True)[:5]:\n",
    "        print (best_positive)\n",
    "    for best_negative in sorted(\n",
    "        feature_to_coef.items(), \n",
    "        key=lambda x: x[1])[:5]:\n",
    "        print (best_negative)\n",
    "        \n",
    "    del reviews_lst_good\n",
    "    del reviews_lst_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:07:18.337Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health_and_Personal_Care_5.json\n",
      "\n",
      "Подбор параметра для:  G:\\Мой диск\\Work\\Experiment\\Health_and_Personal_Care_5.json\n",
      "Accuracy for C=0.01: 0.9375511763742562\n",
      "Accuracy for C=0.05: 0.94242316720345\n",
      "Accuracy for C=0.25: 0.9450161034990993\n",
      "Accuracy for C=0.5: 0.9445930454719144\n",
      "Accuracy for C=1: 0.9438561056826246\n",
      "Final Accuracy: 0.73015\n",
      "('deducted', 1.7656824591106735)\n",
      "('pleasantly', 1.656820137862464)\n",
      "('pleased', 1.6369400592065257)\n",
      "('skeptical', 1.5956846843153587)\n",
      "('highly', 1.5722485210054)\n",
      "('disappointing', -2.5610881508681285)\n",
      "('worthless', -2.4379621815169608)\n",
      "('disappointment', -2.143615023529117)\n",
      "('unacceptable', -2.0601684486255207)\n",
      "('poorly', -1.905376850288279)\n",
      "Movies_and_TV_5.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.listdir('G:\\\\Мой диск\\\\Work\\\\Experiment')\n",
    "\n",
    "for i in os.listdir('G:\\\\Мой диск\\\\Work\\\\Experiment'):\n",
    "    if i[-5:] != 'ipynb' and i != '.ipynb_checkpoints':\n",
    "        print (i)\n",
    "        print ('')\n",
    "        classify_it ('G:\\\\Мой диск\\\\Work\\\\Experiment\\\\' + i, 0.5)\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:23:49.310Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.listdir('G:\\\\Мой диск\\\\Work\\\\Experiment')\n",
    "\n",
    "for i in os.listdir('G:\\\\Мой диск\\\\Work\\\\Experiment'):\n",
    "    if i[-5:] != 'ipynb' and i != '.ipynb_checkpoints':\n",
    "        print (i)\n",
    "        print ('')\n",
    "        classify_it ('G:\\\\Мой диск\\\\Work\\\\Experiment\\\\' + i, 0.25)\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:23:50.132Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def classify_it(filename, parametr): \n",
    "    i = 0\n",
    "    reviews_lst_good= []\n",
    "    reviews_lst_bad = []\n",
    "\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['overall'] > 3 and len(reviews_lst_good)<=500000:\n",
    "                    reviews_lst_good.append (json.loads(line))\n",
    "\n",
    "                elif json.loads(line)['overall'] < 3 and len(reviews_lst_bad)<=500000:\n",
    "                    reviews_lst_bad.append (json.loads(line))\n",
    "\n",
    "                if len(reviews_lst_good) + len (reviews_lst_bad) == 1000000:\n",
    "                    break\n",
    "    reviews_lst_good = random.shuffle(reviews_lst_good)\n",
    "    reviews_lst_bad = random.shuffle(reviews_lst_bad)  \n",
    "    \n",
    "    reviewerID_lst = []\n",
    "    reviewerName_lst = []\n",
    "    reviewText_lst = []\n",
    "    overall_lst = []\n",
    "    summary_lst = []\n",
    "\n",
    "    # reviews_lst [0]['overall']<5 #['asin']for \n",
    "\n",
    "    for i in reviews_lst_good:\n",
    "        reviewerID_lst.append (i['reviewerID'])\n",
    "        reviewText_lst.append (i['reviewText'])\n",
    "        overall_lst.append (i['overall'])\n",
    "        summary_lst.append (i['summary'])\n",
    "\n",
    "    for i in reviews_lst_bad:\n",
    "        reviewerID_lst.append (i['reviewerID'])\n",
    "        reviewText_lst.append (i['reviewText'])\n",
    "        overall_lst.append (i['overall'])\n",
    "        summary_lst.append (i['summary'])\n",
    "\n",
    "    df = pd.DataFrame ()\n",
    "\n",
    "    df['reviewerID'] = reviewerID_lst\n",
    "    # df['reviewerName'] = reviewerName_lst\n",
    "    df['reviewText'] = reviewText_lst\n",
    "    df['overall'] = overall_lst\n",
    "    df['summary'] = summary_lst\n",
    "    \n",
    "    df ['mark'] = 0\n",
    "    df.loc[df['overall'] > 3,'mark'] = 1\n",
    "    \n",
    "    reviews_train = list(df[df['overall'] > 3]['reviewText'][:-10000]) + list(df[df['overall'] < 3]['reviewText'][:-10000])\n",
    "    reviews_test = list(df[df['overall'] > 3]['reviewText'][-10000:]) + list(df[df['overall'] < 3]['reviewText'][-10000:])\n",
    "\n",
    "\n",
    "\n",
    "    reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "    reviews_test_clean = preprocess_reviews(reviews_test)\n",
    "    \n",
    "    cv = TfidfVectorizer(binary=True)\n",
    "    cv.fit(reviews_train_clean)\n",
    "    X = cv.transform(reviews_train_clean)\n",
    "    X_test = cv.transform(reviews_test_clean)\n",
    "    \n",
    "    y = list(df[df['overall'] > 3]['mark'][:-10000]) + list(df[df['overall'] < 3]['mark'][:-10000])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, train_size = 0.75\n",
    "    )\n",
    "    \n",
    "    print ('Подбор параметра для: ', filename)\n",
    "    print ('    ')\n",
    "    \n",
    "    for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "        lr = LogisticRegression(C=c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        print (\"Accuracy for C=%s: %s\" \n",
    "               % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "    \n",
    "    final_model = LogisticRegression(C=parametr)\n",
    "    final_model.fit(X,  y)\n",
    "    print (\"Final Accuracy: %s\" \n",
    "           % accuracy_score( list(df[df['overall'] > 3]['mark'][-10000:]) + list(df[df['overall'] < 3]['mark'][-10000:])\n",
    "                            , final_model.predict(X_test)))\n",
    "    \n",
    "    feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "        )\n",
    "    }\n",
    "    for best_positive in sorted(\n",
    "        feature_to_coef.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True)[:5]:\n",
    "        print (best_positive)\n",
    "    for best_negative in sorted(\n",
    "        feature_to_coef.items(), \n",
    "        key=lambda x: x[1])[:5]:\n",
    "        print (best_negative)\n",
    "        \n",
    "    del reviews_lst_good\n",
    "    del reviews_lst_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:23:51.011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.listdir('G:\\\\Мой диск\\\\Work\\\\Experiment')\n",
    "\n",
    "for i in os.listdir('G:\\\\Мой диск\\\\Work\\\\Experiment'):\n",
    "    if i[-5:] != 'ipynb' and i != '.ipynb_checkpoints':\n",
    "        print (i)\n",
    "        print ('   ')\n",
    "        classify_it ('G:\\\\Мой диск\\\\Work\\\\Experiment\\\\' + i, 0.25)\n",
    "        gc.collect()\n",
    "        print ('   ')\n",
    "        classify_it ('G:\\\\Мой диск\\\\Work\\\\Experiment\\\\' + i, 0.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:23:52.163Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "list = [20, 16, 10, 5];\n",
    "random.shuffle(list)\n",
    "print (\"Reshuffled list : \",  list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:23:53.755Z"
    }
   },
   "outputs": [],
   "source": [
    "# BackUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-08T12:23:54.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json('Books_5.json')\n",
    "# import ijson\n",
    "# filename = \"Books_5.json\"\n",
    "# with open(filename, 'r') as f:\n",
    "#     objects = ijson.items(f, 'meta.view.columns.item')\n",
    "#     columns = list(objects)\n",
    "#     print (objects)\n",
    "# handle = open(\"Books_5.json\", \"r\")\n",
    "\n",
    "# while True:\n",
    "#     data = handle.read(20000)\n",
    "#     if not data:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "179.391px",
    "left": "1521px",
    "right": "20px",
    "top": "117px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
